{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48383927",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Detailed Explanation of the Logistic Regression Algorithm\n",
    "\n",
    "## Introduction to Logistic Regression\n",
    "\n",
    "Logistic regression is a linear model for binary classification problems. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability that a given input point belongs to a certain class (often labeled as 0 or 1).\n",
    "\n",
    "### 1. Sigmoid Function\n",
    "\n",
    "The sigmoid function is a crucial part of logistic regression. It transforms the linear combination of features into a probability value between 0 and 1. The sigmoid function is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $z = X \\theta$, which is a linear combination of input features $X$ and the parameter vector $\\theta$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "\n",
    "The sigmoid function ensures that our output is always between 0 and 1, which can be interpreted as a probability.\n",
    "\n",
    "### 2. Hypothesis Function\n",
    "\n",
    "The hypothesis function for logistic regression uses the sigmoid function to estimate the probability that a given input belongs to class 1. Mathematically, this is represented as:\n",
    "\n",
    "$$\n",
    "h_{\\theta}(x) = \\sigma(X \\theta) = \\frac{1}{1 + e^{-X \\theta}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $h_{\\theta}(x)$ represents the predicted probability of the input belonging to class 1.\n",
    "- $X$ is the feature matrix (with $m$ samples and $n$ features).\n",
    "- $\\theta$ is the parameter vector we aim to learn.\n",
    "\n",
    "### 3. Cost Function (Loss Function)\n",
    "\n",
    "To find the optimal parameters $\\theta$, we minimize a cost function, which measures how well our model's predictions match the actual data. The cost function used in logistic regression is the **negative log-likelihood** (also known as the binary cross-entropy loss):\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $m$ is the total number of training examples.\n",
    "- $y_i$ is the actual label (0 or 1) for the $i$-th training example.\n",
    "- $\\hat{y}_i = h_{\\theta}(x^{(i)}) = \\sigma(x^{(i)} \\cdot \\theta)$ is the predicted probability for the $i$-th training example.\n",
    "\n",
    "The cost function penalizes incorrect predictions. For example:\n",
    "- When $y_i = 1$ and $\\hat{y}_i$ is close to 0, the cost will be high.\n",
    "- When $y_i = 0$ and $\\hat{y}_i$ is close to 1, the cost will also be high.\n",
    "\n",
    "### 4. Gradient Descent for Logistic Regression\n",
    "\n",
    "To minimize the cost function $J(\\theta)$, we use an optimization algorithm called **gradient descent**. Gradient descent iteratively updates the parameter vector $\\theta$ in the direction that reduces the cost function the most.\n",
    "\n",
    "The update rule for $\\theta$ is:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\alpha$ is the learning rate (step size).\n",
    "- $\\nabla J(\\theta)$ is the gradient of the cost function with respect to $\\theta$.\n",
    "\n",
    "The gradient of the cost function for logistic regression is:\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{1}{m} X^T (h_{\\theta}(X) - y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $X$ is the feature matrix.\n",
    "- $h_{\\theta}(X)$ is the vector of predicted probabilities for all samples.\n",
    "- $y$ is the vector of actual labels.\n",
    "\n",
    "### 5. Newton's Method for Optimization\n",
    "\n",
    "Instead of using standard gradient descent, our implementation uses **Newton's method**, which converges faster by incorporating second-order derivative information.\n",
    "\n",
    "Newton's update rule is:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - H^{-1} \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $H$ is the Hessian matrix, which is the second derivative (or curvature) of the cost function with respect to $\\theta$.\n",
    "\n",
    "#### Hessian Matrix Calculation\n",
    "\n",
    "The Hessian matrix $H$ for logistic regression is given by:\n",
    "\n",
    "$$\n",
    "H = \\frac{1}{m} X^T D X + \\lambda I\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $D = \\text{diag}(p_i (1 - p_i))$ is a diagonal matrix where each diagonal element is $p_i (1 - p_i)$, the variance of the Bernoulli random variable for the predicted probability $p_i$.\n",
    "- $\\lambda I$ is a regularization term added to the diagonal to prevent singular matrices, where $\\lambda$ is a small positive constant, and $I$ is the identity matrix.\n",
    "\n",
    "### 6. Regularization\n",
    "\n",
    "To prevent overfitting, we add a regularization term to the Hessian matrix. Regularization helps by penalizing overly complex models (i.e., models with large weights) and encourages simpler models that generalize better.\n",
    "\n",
    "The regularization term $\\lambda I$ controls the magnitude of the penalty:\n",
    "\n",
    "- A small $\\lambda$ results in minimal regularization.\n",
    "- A large $\\lambda$ makes the model simpler (i.e., smaller weight values).\n",
    "\n",
    "### 7. Stopping Criteria\n",
    "\n",
    "The gradient descent (or Newton's method) iterations continue until:\n",
    "\n",
    "1. The change in the parameter vector $\\theta$ is less than a small threshold $\\epsilon$.\n",
    "2. A maximum number of iterations is reached.\n",
    "\n",
    "Mathematically, we stop when:\n",
    "\n",
    "$$\n",
    "\\| \\alpha \\Delta \\theta \\| < \\epsilon\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\| \\cdot \\|$ denotes the norm (magnitude) of the vector.\n",
    "- $\\epsilon$ is a small positive threshold value for convergence.\n",
    "\n",
    "### 8. Making Predictions\n",
    "\n",
    "Once the model is trained (i.e., $\\theta$ is optimized), we can use it to make predictions on new data points.\n",
    "\n",
    "To predict the probability that a new data point $x$ belongs to class 1, we compute:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(x \\cdot \\theta)\n",
    "$$\n",
    "\n",
    "To convert this probability into a binary class label (0 or 1), we use a threshold of 0.5:\n",
    "\n",
    "- If $\\hat{y} \\geq 0.5$, predict 1 (class 1).\n",
    "- If $\\hat{y} < 0.5$, predict 0 (class 0).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This detailed explanation provides a comprehensive understanding of the logistic regression algorithm, including its mathematical foundation, optimization techniques, and practical considerations. By understanding these details, you will be better equipped to interpret and extend your logistic regression model in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43488851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries and preprocess data.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\Machine_Learning\\Downloads\\archive\\Iris.csv')\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\n",
    "y = df['Species'].values\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b21ab",
   "metadata": {},
   "source": [
    "## Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52867d44",
   "metadata": {},
   "source": [
    "# Logistic Regression Implementation from Scratch\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook contains a full implementation of a logistic regression model built from the ground up. The logistic regression algorithm is a popular method used for binary classification tasks. This model uses maximum likelihood estimation to fit a decision boundary between two classes and is trained using gradient descent.\n",
    "\n",
    "### Class Definition\n",
    "\n",
    "We define a Python class, `Logistic Regression`, with methods for initialization, the sigmoid function, fitting the model to training data, computing the loss function, and predicting new data points.\n",
    "\n",
    "\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "The sigmoid function, $\\sigma(z) = \\frac{1}{1 + e^{-z}}$, is used to map predicted values to probabilities between 0 and 1.\n",
    "\n",
    "### Fit Method\n",
    "\n",
    "The fit method implements the core of the logistic regression algorithm using Newton's method for optimization.\n",
    "    x: Feature matrix of shape $(m, n)$, where $m$ is the number of samples and $n$ is the number of features.\n",
    "    y: Target vector of shape $(m, )$, containing binary labels (0 or 1).\n",
    "\n",
    "### Initialization\n",
    "\n",
    "    Adds an intercept term (a column of ones) to x.\n",
    "    Initializes the weight vector theta with small random values if not provided.\n",
    "\n",
    "### Gradient Descent Loop\n",
    "\n",
    "The model is trained using a loop that iteratively adjusts the weights to minimize the cost function.\n",
    "    predictedProbability: The predicted probability of class 1 for each sample using the current model.\n",
    "    errorTerm: The difference between the actual and predicted probabilities.\n",
    "    gradient: The partial derivative of the log-likelihood function with respect to theta, used to update theta in the direction that reduces the loss.\n",
    "\n",
    "The Hessian matrix is computed as:\n",
    "\n",
    "$$\n",
    "H = \\frac{1}{m} X^T D X + \\lambda I\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $D = \\text{diag}(p_i (1 - p_i))$ is a diagonal matrix with predicted probabilities along the diagonal.\n",
    "- $\\lambda I$ is a small regularization term added to stabilize the matrix inversion.\n",
    "\n",
    "### Updating the Parameters\n",
    "    delta_theta: Computed change in parameters. We use np.linalg.solve to solve for delta_theta directly.\n",
    "    If the Hessian is singular, we use the pseudoinverse instead.\n",
    "\n",
    "### Convergence Check\n",
    "The algorithm stops when the parameter updates are smaller than a given threshold eps\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function for logistic regression is the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $y_i$ is the actual label for the $i$-th example.\n",
    "- $\\hat{y}_i$ is the predicted probability for the $i$-th example.\n",
    "- $m$ is the total number of training examples.\n",
    "\n",
    "### Predict Method\n",
    "\n",
    "The predict method outputs class labels based on the predicted probabilities. Adds an intercept term to x.\n",
    "Computes predicted probabilities and thresholds them at 0.5 to determine class labels (0 or 1).\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This implementation of logistic regression uses Newton's method for fast convergence by utilizing both gradient and second-order derivative (Hessian) information. It includes handling of singular matrices using the pseudoinverse, and verbose output to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac3ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, step_size=1, max_iter=10000, eps=1e-5, theta_0=None, verbose=True):\n",
    "        self.theta = theta_0\n",
    "        self.step_size = step_size\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    #step_size: The learning rate, which controls the step size during gradient descent.\n",
    "    #max_iter: The maximum number of iterations for the gradient descent algorithm.\n",
    "    #eps: The convergence threshold for stopping the gradient descent.\n",
    "    #theta_0: Initial parameter values (weights). If not provided, they are initialized randomly.\n",
    "    #verbose: A boolean that determines whether to print progress messages during training.\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500))) #np.clip: Prevents overflow by clipping values of z to be within [-500, 500] before applying the exponential function.\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        m, n = x.shape\n",
    "        if self.theta is None:\n",
    "            self.theta= np.random.randn(n + 1) * 0.01\n",
    "            x = np.concatenate([np.ones((m, 1)), x], axis=1)\n",
    "            \n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            predictedProbability = self.sigmoid(np.dot(x, self.theta))\n",
    "            errorTerm = y-predictedProbability\n",
    "            \n",
    "            gradient = (1/m)*np.dot(x.T,errorTerm)\n",
    "            hessian = ((1/m) * (x.T @ np.diag(predictedProbability * (1 - predictedProbability)) @ x)) + (np.eye(x.shape[1]) * 1e-4)\n",
    "            \n",
    "            try:\n",
    "                delta_theta = np.linalg.solve(hessian, gradient)\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"Singular matrix encountered, using pseudoinverse\")\n",
    "                delta_theta = np.linalg.pinv(hessian).dot(gradient)\n",
    "            \n",
    "            self.theta -= self.step_size * delta_theta\n",
    "\n",
    "            if np.linalg.norm(self.step_size * delta_theta) < self.eps:\n",
    "                break\n",
    "\n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {self.loss(x, y)}\")\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        m = len(y)\n",
    "        predictions = self.sigmoid(np.dot(x, self.theta))\n",
    "        return -np.sum(y * np.log(predictions + 1e-8) + (1 - y) * np.log(1 - predictions + 1e-8)) / m\n",
    "\n",
    "    def predict(self, x):\n",
    "        m,n=x.shape\n",
    "        x = np.concatenate([np.ones((m, 1)), x], axis=1)\n",
    "        probabilities = self.sigmoid(np.dot(x,self.theta))\n",
    "        return (probabilities>=0.5).astype(int)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe12d5",
   "metadata": {},
   "source": [
    "## Training and predicting and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98653d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 5.0385672518496545\n",
      "Iteration 100, Loss: 18.267175071002764\n",
      "Iteration 200, Loss: 18.267175071002764\n",
      "Iteration 300, Loss: 18.267175071002764\n",
      "Iteration 400, Loss: 18.267175071002764\n",
      "Iteration 500, Loss: 18.267175071002764\n",
      "Iteration 600, Loss: 18.267175071002764\n",
      "Iteration 700, Loss: 18.267175071002764\n",
      "Iteration 800, Loss: 18.267175071002764\n",
      "Iteration 900, Loss: 18.267175071002764\n",
      "Iteration 1000, Loss: 18.267175071002764\n",
      "Iteration 1100, Loss: 18.267175071002764\n",
      "Iteration 1200, Loss: 18.267175071002764\n",
      "Iteration 1300, Loss: 18.267175071002764\n",
      "Iteration 1400, Loss: 18.267175071002764\n",
      "Iteration 1500, Loss: 18.267175071002764\n",
      "Iteration 1600, Loss: 18.267175071002764\n",
      "Iteration 1700, Loss: 18.267175071002764\n",
      "Iteration 1800, Loss: 18.267175071002764\n",
      "Iteration 1900, Loss: 18.267175071002764\n",
      "Iteration 2000, Loss: 18.267175071002764\n",
      "Iteration 2100, Loss: 18.267175071002764\n",
      "Iteration 2200, Loss: 18.267175071002764\n",
      "Iteration 2300, Loss: 18.267175071002764\n",
      "Iteration 2400, Loss: 18.267175071002764\n",
      "Iteration 2500, Loss: 18.267175071002764\n",
      "Iteration 2600, Loss: 18.267175071002764\n",
      "Iteration 2700, Loss: 18.267175071002764\n",
      "Iteration 2800, Loss: 18.267175071002764\n",
      "Iteration 2900, Loss: 18.267175071002764\n",
      "Iteration 3000, Loss: 18.267175071002764\n",
      "Iteration 3100, Loss: 18.267175071002764\n",
      "Iteration 3200, Loss: 18.267175071002764\n",
      "Iteration 3300, Loss: 18.267175071002764\n",
      "Iteration 3400, Loss: 18.267175071002764\n",
      "Iteration 3500, Loss: 18.267175071002764\n",
      "Iteration 3600, Loss: 18.267175071002764\n",
      "Iteration 3700, Loss: 18.267175071002764\n",
      "Iteration 3800, Loss: 18.267175071002764\n",
      "Iteration 3900, Loss: 18.267175071002764\n",
      "Iteration 4000, Loss: 18.267175071002764\n",
      "Iteration 4100, Loss: 18.267175071002764\n",
      "Iteration 4200, Loss: 18.267175071002764\n",
      "Iteration 4300, Loss: 18.267175071002764\n",
      "Iteration 4400, Loss: 18.267175071002764\n",
      "Iteration 4500, Loss: 18.267175071002764\n",
      "Iteration 4600, Loss: 18.267175071002764\n",
      "Iteration 4700, Loss: 18.267175071002764\n",
      "Iteration 4800, Loss: 18.267175071002764\n",
      "Iteration 4900, Loss: 18.267175071002764\n",
      "Iteration 5000, Loss: 18.267175071002764\n",
      "Iteration 5100, Loss: 18.267175071002764\n",
      "Iteration 5200, Loss: 18.267175071002764\n",
      "Iteration 5300, Loss: 18.267175071002764\n",
      "Iteration 5400, Loss: 18.267175071002764\n",
      "Iteration 5500, Loss: 18.267175071002764\n",
      "Iteration 5600, Loss: 18.267175071002764\n",
      "Iteration 5700, Loss: 18.267175071002764\n",
      "Iteration 5800, Loss: 18.267175071002764\n",
      "Iteration 5900, Loss: 18.267175071002764\n",
      "Iteration 6000, Loss: 18.267175071002764\n",
      "Iteration 6100, Loss: 18.267175071002764\n",
      "Iteration 6200, Loss: 18.267175071002764\n",
      "Iteration 6300, Loss: 18.267175071002764\n",
      "Iteration 6400, Loss: 18.267175071002764\n",
      "Iteration 6500, Loss: 18.267175071002764\n",
      "Iteration 6600, Loss: 18.267175071002764\n",
      "Iteration 6700, Loss: 18.267175071002764\n",
      "Iteration 6800, Loss: 18.267175071002764\n",
      "Iteration 6900, Loss: 18.267175071002764\n",
      "Iteration 7000, Loss: 18.267175071002764\n",
      "Iteration 7100, Loss: 18.267175071002764\n",
      "Iteration 7200, Loss: 18.267175071002764\n",
      "Iteration 7300, Loss: 18.267175071002764\n",
      "Iteration 7400, Loss: 18.267175071002764\n",
      "Iteration 7500, Loss: 18.267175071002764\n",
      "Iteration 7600, Loss: 18.267175071002764\n",
      "Iteration 7700, Loss: 18.267175071002764\n",
      "Iteration 7800, Loss: 18.267175071002764\n",
      "Iteration 7900, Loss: 18.267175071002764\n",
      "Iteration 8000, Loss: 18.267175071002764\n",
      "Iteration 8100, Loss: 18.267175071002764\n",
      "Iteration 8200, Loss: 18.267175071002764\n",
      "Iteration 8300, Loss: 18.267175071002764\n",
      "Iteration 8400, Loss: 18.267175071002764\n",
      "Iteration 8500, Loss: 18.267175071002764\n",
      "Iteration 8600, Loss: 18.267175071002764\n",
      "Iteration 8700, Loss: 18.267175071002764\n",
      "Iteration 8800, Loss: 18.267175071002764\n",
      "Iteration 8900, Loss: 18.267175071002764\n",
      "Iteration 9000, Loss: 18.267175071002764\n",
      "Iteration 9100, Loss: 18.267175071002764\n",
      "Iteration 9200, Loss: 18.267175071002764\n",
      "Iteration 9300, Loss: 18.267175071002764\n",
      "Iteration 9400, Loss: 18.267175071002764\n",
      "Iteration 9500, Loss: 18.267175071002764\n",
      "Iteration 9600, Loss: 18.267175071002764\n",
      "Iteration 9700, Loss: 18.267175071002764\n",
      "Iteration 9800, Loss: 18.267175071002764\n",
      "Iteration 9900, Loss: 18.267175071002764\n",
      "Validation Accuracy: 0.33\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = clf.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
