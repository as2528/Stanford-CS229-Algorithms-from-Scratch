{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e348e787",
   "metadata": {},
   "source": [
    "# GDA Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e26d16",
   "metadata": {},
   "source": [
    "## Load Necessary Libraries and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "277180e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(path, add_intercept=True):\n",
    "    \"\"\"Load dataset from CSV file.\"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    X = data.iloc[:, 1:-1].values  # Exclude 'Id' and 'Species'\n",
    "    y = data['Species'].values\n",
    "    if add_intercept:\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Add intercept term\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40874159",
   "metadata": {},
   "source": [
    "# GDA Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350179b9",
   "metadata": {},
   "source": [
    "The Gaussian Discriminant Analysis (GDA) algorithm is employed to determine whether a flower belongs to the \"Iris Virginica\" species. In this binary classification task, the label 1 represents \"Iris Virginica,\" while the label 0 denotes any other species. The GDA algorithm is specifically designed for binary classification problems, requiring the class labels to be in a binary format (0 or 1).\n",
    "\n",
    "# Overview of GDA Model Components\n",
    "\n",
    "### 1. Gaussian Distributions for Each Class\n",
    "The GDA model assumes that the data from each class is normally distributed. Therefore, the probability of the features \\( x \\) given the class label \\( y \\) is modeled as a multivariate Gaussian (normal) distribution:\n",
    "\n",
    "$$\n",
    "p(x \\mid y = 0) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu_0)^T \\Sigma^{-1} (x - \\mu_0) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(x \\mid y = 1) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) \\right)\n",
    "$$\n",
    "\n",
    "Here, $\\mu_0$ and $\\mu_1$ are the mean vectors for classes 0 and 1, respectively, and $\\Sigma$ is the shared covariance matrix.\n",
    "\n",
    "### 2. Class Prior Probability ($\\phi$)\n",
    "The prior probability $\\phi$ represents the fraction of the training examples that belong to class 1. It is modeled as a Bernoulli random variable:\n",
    "\n",
    "$$\n",
    "p(y) = \\phi^y (1 - \\phi)^{1 - y}\n",
    "$$\n",
    "\n",
    "### 3. Log-Likelihood of the Data\n",
    "The log-likelihood of the data is the logarithm of the joint probability of the inputs $x^{(i)}$ and outputs $y^{(i)}$:\n",
    "\n",
    "$$\n",
    "\\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\sum_{i=1}^{n} \\log p(x^{(i)}, y^{(i)}; \\phi, \\mu_0, \\mu_1, \\Sigma)\n",
    "$$\n",
    "\n",
    "This can be expanded using the chain rule of probability:\n",
    "\n",
    "$$\n",
    "\\ell(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\sum_{i=1}^{n} \\left[ \\log p(x^{(i)} \\mid y^{(i)}; \\mu_0, \\mu_1, \\Sigma) + \\log p(y^{(i)}; \\phi) \\right]\n",
    "$$\n",
    "\n",
    "# How These Components Come Together in Your Code\n",
    "\n",
    "Now, let's map these equations to the code you're writing for the `fit` method:\n",
    "\n",
    "### 1. Calculate Class Prior ($\\phi$)\n",
    "- **Goal**: Compute $\\phi$, the fraction of training examples that belong to class 1. This is a simple mean calculation over the binary target vector `y_binary`.\n",
    "- **Mathematical formula**: \n",
    "\n",
    "$$\n",
    "\\phi = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}\\{y^{(i)} = 1\\}\n",
    "$$\n",
    "\n",
    "- **Code**: You will compute this using the binary target array, typically with `numpy` operations.\n",
    "\n",
    "### 2. Calculate Means ($\\mu_0, \\mu_1$)\n",
    "- **Goal**: Compute the mean of the input features $x$ for each class. This involves separating the data into two subsets: one for class 0 and one for class 1.\n",
    "- **Mathematical formula**: \n",
    "\n",
    "$$\n",
    "\\mu_0 = \\frac{1}{n_0} \\sum_{i=1}^{n} \\mathbb{1}\\{y^{(i)} = 0\\} x^{(i)}, \\quad \\mu_1 = \\frac{1}{n_1} \\sum_{i=1}^{n} \\mathbb{1}\\{y^{(i)} = 1\\} x^{(i)}\n",
    "$$\n",
    "\n",
    "- **Code**: You will filter the `x` data by `y_binary` to compute $\\mu_0$ and $\\mu_1$.\n",
    "\n",
    "### 3. Calculate Covariance Matrix ($\\Sigma$)\n",
    "- **Goal**: Compute the covariance matrix $\\Sigma$ using all the training examples. This matrix describes the variance and covariance of the features, assuming they share the same covariance across both classes.\n",
    "- **Mathematical formula**: \n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n} \\sum_{i=1}^{n} (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^T\n",
    "$$\n",
    "\n",
    "- **Code**: This involves calculating the deviations of each data point from its corresponding class mean and then computing the sum of these deviations.\n",
    "\n",
    "### 4. Compute Parameters ($\\theta$ and $\\theta_0$)\n",
    "- **Goal**: Derive the parameters that define the decision boundary:\n",
    "\n",
    "$$\n",
    "\\theta = \\Sigma^{-1} (\\mu_1 - \\mu_0)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_0 = -\\frac{1}{2} (\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0) + \\log \\left( \\frac{\\phi}{1 - \\phi} \\right)\n",
    "$$\n",
    "\n",
    "- **Explanation**: The parameters $\\theta$ and $\\theta_0$ define the linear decision boundary that separates the classes in the feature space.\n",
    "\n",
    "# How to Proceed in Your Code\n",
    "\n",
    "1. **First Step**: Convert the target variable `y` to binary values and calculate the class prior $\\phi$.\n",
    "\n",
    "2. **Next Steps**: Implement the calculations for the mean vectors $\\mu_0$ and $\\mu_1$.\n",
    "\n",
    "3. **Further Steps**: Calculate the shared covariance matrix $\\Sigma$.\n",
    "\n",
    "4. **Final Steps**: Derive the parameters $\\theta$ and $\\theta_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d16f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDA:\n",
    "    \"\"\"Gaussian Discriminant Analysis.\"\"\"\n",
    "    def __init__(self, step_size=0.01, max_iter=10000, eps=1e-5,\n",
    "                 theta_0=None, verbose=True):\n",
    "        self.theta = theta_0\n",
    "        self.step_size = step_size\n",
    "        self.max_iter = max_iter\n",
    "        self.eps = eps\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit a GDA model to training set.\"\"\"\n",
    "        \n",
    "        # Step 1: Convert target variable y to binary values\n",
    "        y_binary = np.where(y==\"Iris-virginica\",1,0)\n",
    "        number_of_samples = len(y)\n",
    "        \n",
    "        # Step 2: Calculate class prior phi\n",
    "        self.phi = 1/(number_of_samples)*np.sum(y_binary)#--Mathematically correct\n",
    "        #phi = np.mean(y_binary)\n",
    "        \n",
    "        # Step 3: Filter the data by class labels\n",
    "        mask_0 = y_binary==0 # Filter for class 0, not Iris-Virginica using this mask\n",
    "        mask_1 = y_binary==1# Filter for class 1, Iris-Virginica using this mask\n",
    "        \n",
    "        x_class_0 = x[mask_0] #Filtered classes\n",
    "        x_class_1 = x[mask_1] #Filtered for class\n",
    "        \n",
    "        \n",
    "        # Step 4: Compute the mean vectors for each class\n",
    "        self.mu0 = np.mean(x_class_0, axis=0) #Averaging along the rows of masked x\n",
    "        self.mu1 = np.mean(x_class_1, axis=0) #Averaging along the rows of masked x\n",
    "        \n",
    "        # Step 5: Initialize the covariance matrix\n",
    "        matrix_of_zeros = np.zeros((x.shape[1],x.shape[1])) # Matrix to accumulate outer products\n",
    "        \n",
    "        \n",
    "        # Step 6: Iterate through each sample to compute the covariance matrix\n",
    "        for i, row in enumerate(x):\n",
    "        # Determine the sample's deviation from the respective class mean\n",
    "            if mask_0[i]:\n",
    "            # Sample belongs to class 0, use mu0\n",
    "                matrix_of_zeros+=np.outer(row-self.mu0, row-self.mu0)\n",
    "                \n",
    "            elif mask_1[i]:\n",
    "            # Sample belongs to class 1, use mu1\n",
    "                matrix_of_zeros+=np.outer(row-self.mu1, row-self.mu1)\n",
    "        \n",
    "        self.covariance = matrix_of_zeros/number_of_samples\n",
    "        self.covariance += np.eye(self.covariance.shape[0]) * 1e-5 # Regularization to prevent singular matrix\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"Make a prediction given new inputs x.\"\"\"\n",
    "        self.inverted_covariance = np.linalg.inv(self.covariance) #Invert the covariance matrix\n",
    "        self.theta = self.inverted_covariance@(self.mu1-self.mu0) #Find the theta, the parameter vector\n",
    "        self.theta0 = (-1/2)*((self.mu1.T)@self.inverted_covariance@self.mu1-(self.mu0.T@self.inverted_covariance@self.mu0))+np.log(self.phi/(1-self.phi))#find the intercept term\n",
    "        \n",
    "        self.log_likelihood = x@self.theta+self.theta0#calculate the log-likelihood ratio\n",
    "        \n",
    "        self.sigmoid = 1/(1+np.exp(-self.log_likelihood))#apply the sigmoid function\n",
    "        \n",
    "        \n",
    "         # Classify based on the probability threshold of 0.5\n",
    "        predicted_class = (self.sigmoid >= 0.5).astype(int)\n",
    "    \n",
    "        return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03b61f",
   "metadata": {},
   "source": [
    "### Step 1: Convert Target $ y $ to Binary Values\n",
    "\n",
    "#### Objective\n",
    "\n",
    "We need to transform the target variable $ y $ into binary form. Since we're working with binary classification (e.g., predicting if the flower is \"Iris-virginica\" or not), let's assume that \"Iris-virginica\" is the positive class (class 1) and the rest are negative (class 0).\n",
    "\n",
    "#### Why?\n",
    "\n",
    "This step is crucial because GDA expects the target variable to be in binary format (0 or 1) to compute probabilities for the logistic function.\n",
    "\n",
    "#### Implementation Thought Process\n",
    "\n",
    "1. **Iterate over each label in $ y $:**\n",
    "   - Assign 1 if the label is \"Iris-virginica,\" otherwise 0.\n",
    "\n",
    "#### Checkpoint\n",
    "\n",
    "- Ensure you have the transformed array $ y_{\\text{binary}} $ with values as 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a7c63",
   "metadata": {},
   "source": [
    "### Step 2: Calculate Class Prior $ \\phi $\n",
    "\n",
    "#### Mathematical Explanation\n",
    "\n",
    "To calculate the class prior $ \\phi $, you need to determine the proportion of samples that belong to class 1 (\"Iris-virginica\").\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\phi = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}\\{y^{(i)} = 1\\}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ n $ is the total number of samples in your dataset.\n",
    "- $ \\mathbb{1}\\{y^{(i)} = 1\\} $ is an indicator function that outputs 1 when $ y^{(i)} = 1 $ (meaning \"Iris-virginica\") and 0 otherwise.\n",
    "\n",
    "This formula calculates the fraction of the dataset where the label is `1`.\n",
    "\n",
    "#### Conceptual Steps\n",
    "\n",
    "1. **Count the Number of Positive Class Labels:**\n",
    "   - Sum the total number of samples where $ y = 1 $.\n",
    "\n",
    "2. **Divide by the Total Number of Samples:**\n",
    "   - This gives you the proportion of samples that are labeled as \"Iris-virginica\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d04dac7",
   "metadata": {},
   "source": [
    "### Step 3: Calculate Mean Vectors $ \\mu_0 $ and $ \\mu_1 $\n",
    "\n",
    "#### Objective\n",
    "\n",
    "The mean vectors $ \\mu_0 $ and $ \\mu_1 $ represent the average values of the input features $ x $ for each class:\n",
    "\n",
    "- $ \\mu_0 $ is the mean vector for class 0 (not \"Iris-virginica\").\n",
    "- $ \\mu_1 $ is the mean vector for class 1 (\"Iris-virginica\").\n",
    "\n",
    "These mean vectors help define the Gaussian distribution for each class, which is a core part of the GDA model.\n",
    "\n",
    "#### Mathematical Formulas\n",
    "\n",
    "$$\n",
    "\\mu_0 = \\frac{1}{n_0} \\sum_{i=1}^{n} \\mathbb{1}\\{y^{(i)} = 0\\} \\, x^{(i)}, \\quad \\mu_1 = \\frac{1}{n_1} \\sum_{i=1}^{n} \\mathbb{1}\\{y^{(i)} = 1\\} \\, x^{(i)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ n_0 $ is the number of samples where $ y = 0 $ (not \"Iris-virginica\").\n",
    "- $ n_1 $ is the number of samples where $ y = 1 $ (\"Iris-virginica\").\n",
    "- $ \\mathbb{1}\\{y^{(i)} = 0\\} $ and $ \\mathbb{1}\\{y^{(i)} = 1\\} $ are indicator functions that pick out samples belonging to class 0 or class 1, respectively.\n",
    "- $ x^{(i)} $ is the feature vector for the $ i $-th sample.\n",
    "\n",
    "#### Conceptual Steps\n",
    "\n",
    "1. **Filter the Data:**\n",
    "   - Separate the data $ x $ into two subsets: one for samples where $ y = 0 $ (class 0) and another for samples where $ y = 1 $ (class 1).\n",
    "\n",
    "2. **Calculate the Mean for Each Subset:**\n",
    "   - Compute the mean vector for each subset. The mean vector $ \\mu_0 $ should be the average of all samples in class 0, and $ \\mu_1 $ should be the average of all samples in class 1.\n",
    "\n",
    "#### Implementation Guidance\n",
    "\n",
    "1. **Filter `x` by `y_binary`:**\n",
    "   - Use logical indexing or masking to create two subsets: `x` where `y_binary == 0` and `x` where `y_binary == 1`.\n",
    "\n",
    "2. **Compute the Means:**\n",
    "   - Calculate the mean of each subset along the appropriate axis (e.g., along the rows if `x` is a 2D array)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d349659",
   "metadata": {},
   "source": [
    "### Step 4: Calculating the Covariance Matrix $ \\Sigma $\n",
    "\n",
    "#### Objective\n",
    "\n",
    "To compute the shared covariance matrix $ \\Sigma $, which defines the shape of the Gaussian distribution for each class. This matrix captures the variance and covariance of the features across all samples, assuming the same covariance for both classes.\n",
    "\n",
    "#### Mathematical Formula\n",
    "\n",
    "The covariance matrix $ \\Sigma $ is given by:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n} \\sum_{i=1}^{n} \\left( x^{(i)} - \\mu_{y^{(i)}} \\right) \\left( x^{(i)} - \\mu_{y^{(i)}} \\right)^T\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ n $ is the total number of samples.\n",
    "- $ x^{(i)} $ is the feature vector of the $ i $-th sample.\n",
    "- $ \\mu_{y^{(i)}} $ is the mean vector corresponding to the class label $ y^{(i)} $:\n",
    "  - If $ y^{(i)} = 0 $, use $ \\mu_0 $.\n",
    "  - If $ y^{(i)} = 1 $, use $ \\mu_1 $.\n",
    "\n",
    "#### Conceptual Steps\n",
    "\n",
    "1. **Compute the Deviations:**\n",
    "   - For each sample $ x^{(i)} $, calculate the deviation from its corresponding class mean:\n",
    "     - $ x^{(i)} - \\mu_0 $ for class 0 samples.\n",
    "     - $ x^{(i)} - \\mu_1 $ for class 1 samples.\n",
    "\n",
    "2. **Sum the Outer Products:**\n",
    "   - For each sample, compute the outer product of the deviation vector with itself and sum these matrices over all samples.\n",
    "\n",
    "3. **Normalize by the Total Number of Samples:**\n",
    "   - Divide the summed matrix by $ n $ (the total number of samples) to compute the final shared covariance matrix $ \\Sigma $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c1fbd",
   "metadata": {},
   "source": [
    "### Implementation of the `predict` Method\n",
    "\n",
    "The goal of the `predict` method is to use the parameters learned during the `fit` method to predict whether new input data $x$ belongs to class 1 (\"Iris-virginica\") or class 0 (not \"Iris-virginica\").\n",
    "\n",
    "#### Steps to Implement `predict`\n",
    "\n",
    "1. **Compute the Decision Boundary:**\n",
    "   - Use the parameters ($self.phi$, $self.mu0$, $self.mu1$, $self.covariance$) learned during the `fit` method to compute the decision boundary for each sample.\n",
    "   - Calculate the log-likelihood ratio or the linear decision boundary for each sample to determine the class probabilities.\n",
    "\n",
    "2. **Calculate the Log-Likelihood Ratio:**\n",
    "   - For each sample, compute the log-likelihood ratio of it belonging to class 1 (\"Iris-virginica\") versus class 0 (not \"Iris-virginica\").\n",
    "   - This involves using the inverse of the covariance matrix ($self.covariance$) and the mean vectors ($self.mu0$, $self.mu1$).\n",
    "\n",
    "3. **Convert to Probabilities:**\n",
    "   - Apply the logistic (sigmoid) function to the log-likelihood ratio to convert it to a probability:\n",
    "\n",
    "   $$\n",
    "   p = \\frac{1}{1 + \\exp(-\\text{log-likelihood})}\n",
    "   $$\n",
    "\n",
    "   - This gives the probability of each sample belonging to class 1.\n",
    "\n",
    "4. **Make Predictions:**\n",
    "   - For each sample, if the probability $p \\geq 0.5$, classify it as \"Iris-virginica\" (class 1). Otherwise, classify it as not \"Iris-virginica\" (class 0).\n",
    "\n",
    "#### Mathematical Details for Log-Likelihood Ratio\n",
    "\n",
    "The log-likelihood ratio $\\text{log}(p(x \\mid y = 1)) - \\text{log}(p(x \\mid y = 0))$ can be computed as:\n",
    "\n",
    "$$\n",
    "\\text{log-likelihood} = x \\cdot \\theta\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x$ is the input data (make sure to add an intercept term if needed).\n",
    "- $\\theta = \\Sigma^{-1} (\\mu_1 - \\mu_0)$ is the parameter vector computed from the inverse of the covariance matrix and the difference between the mean vectors.\n",
    "- $\\theta_0 = -\\frac{1}{2} (\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0) + \\log \\left( \\frac{\\phi}{1 - \\phi} \\right)$ is the intercept term.\n",
    "\n",
    "#### Implementation Strategy\n",
    "\n",
    "1. **Compute the Parameter Vector ($\\theta$):**\n",
    "   - Use the inverse of $self.covariance$ and the difference between $self.mu1$ and $self.mu0$.\n",
    "\n",
    "2. **Compute the Intercept ($\\theta_0$):**\n",
    "   - Calculate using the mean vectors, the covariance matrix, and the class prior $self.phi$.\n",
    "\n",
    "3. **Calculate the Log-Likelihood Ratio:**\n",
    "   - For each input sample, compute the linear decision boundary.\n",
    "\n",
    "4. **Apply the Sigmoid Function:**\n",
    "   - Convert the decision boundary to a probability.\n",
    "\n",
    "5. **Classify Based on Probability:**\n",
    "   - Use a threshold of 0.5 to make the final classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f18165",
   "metadata": {},
   "source": [
    "### Implementation of the `predict` Method\n",
    "\n",
    "The goal of the `predict` method is to use the parameters learned during the `fit` method to predict whether new input data $x$ belongs to class 1 (\"Iris-virginica\") or class 0 (not \"Iris-virginica\").\n",
    "\n",
    "#### Steps to Implement `predict`\n",
    "\n",
    "1. **Compute the Decision Boundary:**\n",
    "   - Use the parameters ($self.phi$, $self.mu0$, $self.mu1$, $self.covariance$) learned during the `fit` method to compute the decision boundary for each sample.\n",
    "   - Calculate the log-likelihood ratio or the linear decision boundary for each sample to determine the class probabilities.\n",
    "\n",
    "2. **Calculate the Log-Likelihood Ratio:**\n",
    "   - For each sample, compute the log-likelihood ratio of it belonging to class 1 (\"Iris-virginica\") versus class 0 (not \"Iris-virginica\").\n",
    "   - This involves using the inverse of the covariance matrix ($self.covariance$) and the mean vectors ($self.mu0$, $self.mu1$).\n",
    "\n",
    "3. **Convert to Probabilities:**\n",
    "   - Apply the logistic (sigmoid) function to the log-likelihood ratio to convert it to a probability:\n",
    "\n",
    "   $$\n",
    "   p = \\frac{1}{1 + \\exp(-\\text{log-likelihood})}\n",
    "   $$\n",
    "\n",
    "   - This gives the probability of each sample belonging to class 1.\n",
    "\n",
    "4. **Make Predictions:**\n",
    "   - For each sample, if the probability $p \\geq 0.5$, classify it as \"Iris-virginica\" (class 1). Otherwise, classify it as not \"Iris-virginica\" (class 0).\n",
    "\n",
    "#### Mathematical Details for Log-Likelihood Ratio\n",
    "\n",
    "The log-likelihood ratio $\\text{log}(p(x \\mid y = 1)) - \\text{log}(p(x \\mid y = 0))$ can be computed as:\n",
    "\n",
    "$$\n",
    "\\text{log-likelihood} = x \\cdot \\theta + \\theta_0\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x$ is the input data (make sure to add an intercept term if needed).\n",
    "- $\\theta = \\Sigma^{-1} (\\mu_1 - \\mu_0)$ is the parameter vector computed from the inverse of the covariance matrix and the difference between the mean vectors.\n",
    "- $\\theta_0 = -\\frac{1}{2} (\\mu_1^T \\Sigma^{-1} \\mu_1 - \\mu_0^T \\Sigma^{-1} \\mu_0) + \\log \\left( \\frac{\\phi}{1 - \\phi} \\right)$ is the intercept term.\n",
    "\n",
    "#### Implementation Strategy\n",
    "\n",
    "1. **Compute the Parameter Vector ($\\theta$):**\n",
    "   - Use the inverse of $self.covariance$ and the difference between $self.mu1$ and $self.mu0$.\n",
    "\n",
    "2. **Compute the Intercept ($\\theta_0$):**\n",
    "   - Calculate using the mean vectors, the covariance matrix, and the class prior $self.phi$.\n",
    "\n",
    "3. **Calculate the Log-Likelihood Ratio:**\n",
    "   - For each input sample, compute the linear decision boundary.\n",
    "\n",
    "4. **Apply the Sigmoid Function:**\n",
    "   - Convert the decision boundary to a probability.\n",
    "\n",
    "5. **Classify Based on Probability:**\n",
    "   - Use a threshold of 0.5 to make the final classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02151ead",
   "metadata": {},
   "source": [
    "# Print out accuracy of the algorithm on the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "375b144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8000\n",
      "Predicted Probabilities:\n",
      "1.0000\n",
      "1.0000\n",
      "0.0000\n",
      "1.0000\n",
      "1.0000\n",
      "0.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "0.0000\n",
      "0.0000\n",
      "1.0000\n",
      "1.0000\n",
      "0.0000\n",
      "0.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n",
      "1.0000\n"
     ]
    }
   ],
   "source": [
    "def main(data_path):\n",
    "    \"\"\"Problem: Gaussian discriminant analysis (GDA)\"\"\"\n",
    "    # Load dataset\n",
    "    x, y = load_dataset(data_path, add_intercept=True)\n",
    "\n",
    "    # Split dataset into training and validation\n",
    "    split_index = int(0.8 * x.shape[0])\n",
    "    x_train, x_valid = x[:split_index], x[split_index:]\n",
    "    y_train, y_valid = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Train GDA classifier\n",
    "    clf = GDA()\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    probabilities = clf.predict(x_valid)\n",
    "    predictions = np.where(probabilities >= 0.5, 'Iris-virginica', 'Iris-setosa')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predictions == y_valid)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Print predicted probabilities\n",
    "    print(\"Predicted Probabilities:\")\n",
    "    for prob in probabilities:\n",
    "        print(f\"{prob:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(data_path='C:/Users/Machine_Learning/Downloads/archive/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db3a37",
   "metadata": {},
   "source": [
    "Reasons for Differences in Accuracy\n",
    "\n",
    "    Covariance Matrix Calculation:\n",
    "                    Manual computation might introduce inaccuracies due to floating-point arithmetic errors, especially with larger datasets. Using np.cov is more reliable and efficient.\n",
    "\n",
    "    Use of Pseudo-Inverse:\n",
    "        The pseudo-inverse (np.linalg.pinv) is more robust against numerical instability and near-singular matrices. The use of the regular inverse (np.linalg.inv) could cause large errors if the covariance matrix is not perfectly invertible.\n",
    "\n",
    "    Data Preprocessing Consistency:\n",
    "        Ensure that the input data x in both fit and predict is processed similarly (adding an intercept term, etc.). Inconsistent preprocessing can lead to mismatches in dimensions and incorrect predictions.\n",
    "\n",
    "    Regularization and Stability:\n",
    "        Regularization is crucial to prevent overfitting and to ensure the covariance matrix is not singular. Using np.cov with regularization and a stable matrix inversion (np.linalg.pinv) will yield more accurate results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
