{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e793986-10fd-4a91-b7eb-0a7df612d262",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbab69f-e654-4732-af9b-a422784fcc58",
   "metadata": {},
   "source": [
    "### Context of the Dataset\n",
    "\n",
    "- The model is trained on the **Iris dataset**, which includes samples from three types of iris flowers: *Iris-setosa*, *Iris-versicolor*, and *Iris-virginica*.\n",
    "- This implementation converts the problem into a **binary classification task**:\n",
    "  - **Class 0:** *Iris-setosa*\n",
    "  - **Class 1:** \"Not Iris-setosa\" (either *Iris-versicolor* or *Iris-virginica*)\n",
    "\n",
    "### How the Perceptron Model Makes Predictions\n",
    "\n",
    "1. **Training Process:**\n",
    "   - The perceptron model is trained on a subset of the dataset (`X_train`, `y_train`), adjusting its weights and bias to minimize errors in classifying samples as either *Iris-setosa* (0) or \"not Iris-setosa\" (1).\n",
    "\n",
    "2. **Prediction:**\n",
    "   - The `predict` method uses the learned weights and bias to classify new samples in `X_test`:\n",
    "     - **0:** If the sample is predicted to be *Iris-setosa*.\n",
    "     - **1:** If the sample is predicted to be either *Iris-versicolor* or *Iris-virginica*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb03fcc-d822-4da9-8c13-b0254f71b77f",
   "metadata": {},
   "source": [
    "### How the `fit` Method Works\n",
    "\n",
    "The `fit` method trains the perceptron by adjusting the weights and bias to minimize the classification error over a specified number of iterations (`self.n_iters`).\n",
    "\n",
    "#### Step-by-Step Breakdown\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The weights (`self.weights`) are initialized to zeros (or small random values) with a length equal to the number of features in your dataset.\n",
    "   - The bias (`self.bias`) is initialized to zero. These initial values provide a starting point for learning.\n",
    "\n",
    "\n",
    "2. **Outer Loop (`for i in range(self.n_iters)`):**\n",
    "   - This loop runs for a specified number of iterations (`self.n_iters`). Each iteration represents one complete pass over the entire training dataset.\n",
    "   - The purpose of this loop is to train the model over multiple passes to ensure it converges to a solution that correctly separates the classes.\n",
    "\n",
    "\n",
    "3. **Inner Loop (`for idx, x_i in enumerate(X)`):**\n",
    "   - The inner loop iterates over each sample (`x_i`) in the training dataset (`X`) during each iteration of the outer loop.\n",
    "   - `x_i` represents a single training sample (a vector of feature values), and `idx` is its corresponding index, which is used to access the correct target value (`y[idx]`).\n",
    "\n",
    "\n",
    "4. **Calculate the Linear Combination (`z`):**\n",
    "   - The perceptron computes a weighted sum of the input features plus the bias:\n",
    "     $$\n",
    "     z = \\text{self.weights} \\cdot x_i + \\text{self.bias}\n",
    "     $$\n",
    "   - Here, `self.weights @ x_i` is the dot product between the weight vector (`self.weights`) and the feature vector (`x_i`), resulting in a scalar value (`z`) representing the linear combination of inputs weighted by the current model weights.\n",
    "   - Adding the bias (`self.bias`) shifts this linear combination to adjust the decision boundary.\n",
    "\n",
    "\n",
    "5. **Apply the Step Function to Predict Output (`output`):**\n",
    "   - The step function (`self._step_function(z)`) is applied to `z` to generate the predicted class label (`output`):\n",
    "     - If `z >= 0`, the output is `1`.\n",
    "     - If `z < 0`, the output is `0`.\n",
    "   - This step function determines which side of the decision boundary the input `x_i` falls on.\n",
    "\n",
    "\n",
    "6. ### Calculate the Error and Update the Weights and Bias\n",
    "\n",
    "The error is computed as the difference between the actual class label (`y[idx]`) and the predicted class label (`output`):\n",
    "\n",
    "$$\n",
    "\\text{error} = y[\\text{idx}] - \\text{output}\n",
    "$$\n",
    "\n",
    "The weights are updated proportionally to the input feature vector and the error:\n",
    "\n",
    "$$\n",
    "\\text{self.weights} += \\text{self.learning\\_rate} \\times \\text{error} \\times x_i\n",
    "$$\n",
    "\n",
    "If the prediction (`output`) is incorrect, the weights are adjusted to reduce future errors.\n",
    "\n",
    "The bias is updated similarly:\n",
    "\n",
    "$$\n",
    "\\text{self.bias} += \\text{self.learning\\_rate} \\times \\text{error}\n",
    "$$\n",
    "$$\n",
    "\n",
    "\n",
    "7. **Repeat Until Convergence or Completion of Iterations:**\n",
    "   - The process repeats for all samples (`x_i`) in each iteration (`self.n_iters`), adjusting the weights and bias to minimize classification error.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- The `fit` method aims to adjust the weights and bias so that the perceptron correctly classifies as many samples as possible from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fce9bdc-0e29-47e6-99fb-415ac8e2344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Weights: [ 0.007  0.049 -0.066 -0.036]\n",
      "Bias: 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset from the specified file path\n",
    "df = pd.read_csv(r'C:\\Users\\Machine-Learning\\Downloads\\iris\\iris.data', header=None)\n",
    "\n",
    "# Assign column names to the dataset\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "# Convert class labels to numerical values\n",
    "df['class'] = df['class'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
    "\n",
    "# Prepare the data for binary classification (e.g., \"Iris-setosa\" vs. \"not Iris-setosa\")\n",
    "# You can also extend this to a multi-class OvR setup\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].values\n",
    "y = (df['class'] == 0).astype(int).values  # 1 for \"Iris-setosa\", 0 for \"not Iris-setosa\"\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perceptron Implementation\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = np.zeros(4)\n",
    "        self.bias = 0\n",
    "        \n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        i=0 \n",
    "        for i in range(self.n_iters): # Each iteration represents one complete pass over the entire training dataset.\n",
    "            for idx, x_i in enumerate(X): # This loop will run over all samples in the dataset idx=index, x_i=ith sample\n",
    "                z=self.weights@x_i+self.bias # Calculation of z by calculating weights and x and adding bias term\n",
    "                output = self._step_function(z) # Calling the step function to decide to output 1/0\n",
    "\n",
    "                self.weights += self.learning_rate*(y[idx]-output)*x_i # updating the weights\n",
    "                self.bias += self.learning_rate*(y[idx]-output) #updating the bias\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        z=X@self.weights+self.bias\n",
    "        return self._step_function(z)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _step_function(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "# Initialize and train the Perceptron model\n",
    "perceptron = Perceptron(learning_rate=0.01, n_iters=1000) #Instantiate a perceptron\n",
    "perceptron.fit(X_train, y_train) #Fit to the dataset\n",
    "# Make predictions\n",
    "y_pred = perceptron.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Display weights and bias\n",
    "print(f\"Weights: {perceptron.weights}\")\n",
    "print(f\"Bias: {perceptron.bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de8d77-5365-4f30-b821-40e1ee73f48a",
   "metadata": {},
   "source": [
    "## Explanation of the Perceptron Model's Predictions\n",
    "\n",
    "### Steps for Prediction\n",
    "\n",
    "1. **Compute the Linear Combination (`z`):**\n",
    "\n",
    "For each test sample, the perceptron calculates a linear combination (`z`) of the input features and the learned weights, plus the bias:\n",
    "\n",
    "$$\n",
    "z = X \\cdot \\text{self.weights} + \\text{self.bias}\n",
    "$$\n",
    "\n",
    "This linear combination represents the perceptron's decision boundary: if `z` is greater than or equal to 0, the perceptron predicts class 1; if `z` is less than 0, it predicts class 0.\n",
    "\n",
    "2. **Apply the Step Function:**\n",
    "\n",
    "The step function converts this linear combination into a binary decision:\n",
    "- **1** (\"not Iris-setosa\") if $z \\geq 0$\n",
    "- **0** (*Iris-setosa*) if $z < 0$\n",
    "\n",
    "3. **Return Predicted Labels:**\n",
    "\n",
    "The predicted class labels (`y_pred`) for each sample in `X_test` are the outputs of this step function.\n",
    "\n",
    "### Interpretation of Results\n",
    "\n",
    "- **Accuracy:** \n",
    "  - The accuracy score calculated using `accuracy_score(y_test, y_pred)` represents the percentage of correct predictions the model made on the test data. For example, an accuracy of 85% means that 85% of the samples in the test set were correctly classified.\n",
    "\n",
    "- **Weights and Bias:**\n",
    "  - The **weights** (`self.weights`) show the importance assigned to each feature when making predictions. Higher absolute values indicate greater influence of that feature on the classification decision.\n",
    "  - The **bias** (`self.bias`) shifts the decision boundary to better fit the training data, helping the perceptron decide the classification threshold.\n",
    "\n",
    "### Summary of Predictions\n",
    "\n",
    "- The perceptron model learns to classify iris flowers as either *Iris-setosa* or \"not Iris-setosa\" based on the provided features (sepal length, sepal width, petal length, petal width).\n",
    "- The predicted labels (`y_pred`) represent the model's classification of the test set samples, and the accuracy score indicates the model's performance in making these predictions.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Evaluate Model Performance:** If the accuracy is high, the model is performing well. If not, consider adjusting hyperparameters like the learning rate or number of iterations, or experiment with different features.\n",
    "- **Experiment with More Data:** Use different subsets or perform cross-validation to assess the model's generalization.\n",
    "- **Extend to Multi-class Classification:** Consider extending the perceptron to handle all three classes using a one-vs-rest approach, where multiple perceptrons are trained, one for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97251b64-6ab8-49bb-8ab0-54559d0d8a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
