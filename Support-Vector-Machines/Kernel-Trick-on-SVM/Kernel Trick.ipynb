{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7ead7f6-4326-4650-a65a-230d29929e03",
   "metadata": {},
   "source": [
    "# Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d74781-8afd-481f-80c5-9404d6c4e81d",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) with Kernel Trick: Algorithm Overview\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Support Vector Machine (SVM)\n",
    "SVM is a supervised learning algorithm used for binary classification. It aims to find the optimal hyperplane that maximizes the margin between two classes in the feature space. For non-linearly separable data, SVM uses the **kernel trick** to project the data into a higher-dimensional space where it becomes linearly separable.\n",
    "\n",
    "### 2. Kernel Trick\n",
    "The kernel trick allows us to compute the inner product in the high-dimensional feature space without explicitly transforming the data. Instead, a **kernel function** is used to compute the similarity between data points.\n",
    "\n",
    "#### Gaussian RBF Kernel\n",
    "The Gaussian Radial Basis Function (RBF) kernel is defined as:\n",
    "$\n",
    "K(x_1, x_2) = \\exp\\left(-\\frac{\\|x_1 - x_2\\|^2}{2\\sigma^2}\\right)\n",
    "$\n",
    "This kernel measures the similarity between two data points, with $\\sigma$ controlling the width of the Gaussian.\n",
    "\n",
    "### 3. Dual Formulation\n",
    "In the dual form of SVM, the optimization problem involves Lagrange multipliers ($\\alpha_i$) and the kernel function. The decision function is expressed as:\n",
    "$\n",
    "f(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n",
    "$\n",
    "where:\n",
    "- $x_i$: Training samples\n",
    "- $y_i$: Corresponding labels ($-1$ or $1$)\n",
    "- $K(x_i, x)$: Kernel function\n",
    "- $b$: Bias term\n",
    "\n",
    "## Algorithm Steps\n",
    "\n",
    "### 1. Define the Kernel Function\n",
    "The RBF kernel computes the similarity between two vectors $x_1$ and $x_2$ using the Gaussian formula.\n",
    "\n",
    "### 2. Initialize Parameters\n",
    "- $\\alpha$: Array of Lagrange multipliers, initialized to zeros.\n",
    "- $b$: Bias term, initialized to zero.\n",
    "- $C$: Regularization parameter to control the trade-off between maximizing the margin and minimizing the hinge loss.\n",
    "\n",
    "### 3. Compute the Kernel Matrix\n",
    "The kernel matrix $K$ is computed for all pairs of training samples:\n",
    "$\n",
    "K[i, j] = K(x_i, x_j)\n",
    "$\n",
    "This precomputes the similarity between each pair of points in the training set.\n",
    "\n",
    "### 4. Train the Model using Gradient Descent\n",
    "For each training sample:\n",
    "1. Compute the decision function:\n",
    "   $\n",
    "   f(x_i) = \\sum_{j=1}^n \\alpha_j y_j K(x_j, x_i) + b\n",
    "   $\n",
    "2. Check the condition for the hinge loss:\n",
    "   $\n",
    "   y_i f(x_i) < 1\n",
    "   $\n",
    "   - If the condition is satisfied, update $\\alpha_i$ to minimize the hinge loss and regularization term.\n",
    "   - Otherwise, update $\\alpha_i$ to minimize the regularization term.\n",
    "\n",
    "3. Update the bias term $b$ to minimize the overall loss.\n",
    "\n",
    "### 5. Make Predictions\n",
    "For a test sample $x$, predict the class using the decision function:\n",
    "$\n",
    "\\hat{y} = \\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\\right)\n",
    "$\n",
    "\n",
    "## Example Workflow\n",
    "1. Define the kernel function (e.g., RBF kernel).\n",
    "2. Initialize the SVM with kernel, regularization parameter $C$, learning rate, and number of iterations.\n",
    "3. Train the SVM on the training dataset to optimize $\\alpha$ and $b$.\n",
    "4. Use the trained SVM to predict labels for new test data.\n",
    "\n",
    "## Limitations\n",
    "- The algorithm uses gradient descent for optimizing $\\alpha$, which may not be as efficient as Quadratic Programming (QP) methods used in traditional SVM implementations.\n",
    "- Computational complexity increases with the size of the training set due to the kernel matrix computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273055bc-025d-47ac-8053-3a69cdc52ef0",
   "metadata": {},
   "source": [
    "# Why Do We Use a Kernel in SVM?\n",
    "\n",
    "## Key Motivation\n",
    "In many real-world datasets, the classes are not linearly separable in the original feature space. For instance, imagine a dataset where the positive and negative classes form concentric circles. In such cases, no straight line or hyperplane can separate the two classes in their original 2D space.\n",
    "\n",
    "## What Does a Kernel Do?\n",
    "\n",
    "### 1. Maps Data to a Higher-Dimensional Space\n",
    "The kernel function allows us to transform the data into a higher-dimensional space where the classes become linearly separable. This transformation is defined by a **feature mapping function** $\\phi(x)$:\n",
    "$$\n",
    "x \\mapsto \\phi(x)\n",
    "$$\n",
    "\n",
    "### 2. Avoids Explicit Computation in High Dimensions\n",
    "Instead of explicitly transforming the data using $\\phi(x)$ (which could be computationally expensive or infeasible for infinite-dimensional spaces), the kernel computes the dot product in the transformed space directly:\n",
    "$$\n",
    "K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\n",
    "$$\n",
    "This means we don't need to know the mapping $\\phi(x)$ explicitly. The kernel function computes the similarity between points as if they were in the high-dimensional space.\n",
    "\n",
    "### 3. Measures Similarity in High-Dimensional Space\n",
    "- In the original space, two points $x_1$ and $x_2$ might seem far apart.\n",
    "- In the transformed space, their similarity (dot product) might indicate that they are close to each other in the new feature space.\n",
    "\n",
    "### 4. Transforms the Decision Boundary\n",
    "- In the original space, the decision boundary is a linear hyperplane.\n",
    "- In the transformed space, the decision boundary becomes non-linear when projected back into the original space.\n",
    "\n",
    "For example, using an RBF kernel, the SVM can create circular, elliptical, or more complex decision boundaries.\n",
    "\n",
    "## Why Not Transform the Data Explicitly?\n",
    "Explicitly transforming the data into a high-dimensional space can be:\n",
    "- **Computationally expensive**, especially for large datasets.\n",
    "- **Memory-intensive**, as higher dimensions require storing large matrices.\n",
    "- **Unnecessary**, since the kernel computes the inner product directly, bypassing the need for explicit transformations.\n",
    "\n",
    "## Common Kernel Functions\n",
    "\n",
    "### 1. Linear Kernel\n",
    "$$\n",
    "K(x_1, x_2) = x_1 \\cdot x_2\n",
    "$$\n",
    "Used for linearly separable problems; no transformation is applied.\n",
    "\n",
    "### 2. Polynomial Kernel\n",
    "$$\n",
    "K(x_1, x_2) = (\\gamma x_1 \\cdot x_2 + r)^d\n",
    "$$\n",
    "Introduces polynomial relationships between features.\n",
    "\n",
    "### 3. Gaussian RBF Kernel\n",
    "$$\n",
    "K(x_1, x_2) = \\exp\\left(-\\frac{\\|x_1 - x_2\\|^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "Maps data into an infinite-dimensional space, making it possible to separate complex datasets.\n",
    "\n",
    "### 4. Sigmoid Kernel\n",
    "$$\n",
    "K(x_1, x_2) = \\tanh(\\gamma x_1 \\cdot x_2 + r)\n",
    "$$\n",
    "Mimics the behavior of a neural network activation function.\n",
    "\n",
    "## Summary\n",
    "The kernel function is a powerful tool that allows SVMs to handle non-linear problems by:\n",
    "1. Implicitly transforming data into a higher-dimensional space.\n",
    "2. Computing similarities in the transformed space efficiently.\n",
    "3. Enabling the creation of non-linear decision boundaries in the original space.\n",
    "\n",
    "By using kernels, SVMs become a versatile tool for both linear and non-linear classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91f5cd-076c-4e7a-a4a9-b5b89bafec81",
   "metadata": {},
   "source": [
    "# Understanding and Implementing the Gaussian RBF Kernel\n",
    "\n",
    "## Mathematical Definition\n",
    "The RBF kernel is defined as:\n",
    "$$\n",
    "K(x_1, x_2) = \\exp\\left(-\\frac{\\|x_1 - x_2\\|^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "### Breakdown of the Formula\n",
    "1. **Euclidean Distance**:\n",
    "   $$\n",
    "   \\|x_1 - x_2\\|^2\n",
    "   $$\n",
    "   This term measures how far apart the two vectors $x_1$ and $x_2$ are in the input space.\n",
    "\n",
    "2. **Gaussian Scaling**:\n",
    "   $$\n",
    "   \\frac{\\|x_1 - x_2\\|^2}{2\\sigma^2}\n",
    "   $$\n",
    "   - $\\sigma$ is the standard deviation of the Gaussian function.\n",
    "   - It controls how quickly the similarity decreases as the distance between the points increases.\n",
    "   - Smaller $\\sigma$ results in a sharper drop-off in similarity, while larger $\\sigma$ makes the kernel smoother.\n",
    "\n",
    "3. **Exponentiation**:\n",
    "   $$\n",
    "   \\exp\\left(-\\frac{\\|x_1 - x_2\\|^2}{2\\sigma^2}\\right)\n",
    "   $$\n",
    "   The exponential function ensures the kernel outputs a similarity score in the range $(0, 1]$, where:\n",
    "   - $K(x_1, x_2) = 1$ when $x_1 = x_2$.\n",
    "   - $K(x_1, x_2) \\to 0$ as $\\|x_1 - x_2\\| \\to \\infty$.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Plan\n",
    "\n",
    "### Steps to Implement\n",
    "1. **Compute the Distance**:\n",
    "   - Use the Euclidean norm $\\|x_1 - x_2\\|$.\n",
    "   - Square the distance to align with the formula.\n",
    "\n",
    "2. **Scale by $\\sigma$**:\n",
    "   - Divide the squared distance by $2\\sigma^2$.\n",
    "   - $\\sigma$ acts as a hyperparameter; it controls how sensitive the kernel is to differences in data points.\n",
    "\n",
    "3. **Apply the Exponential Function**:\n",
    "   - Use the exponential function to compute the similarity score.\n",
    "\n",
    "---\n",
    "\n",
    "## Debugging and Validation\n",
    "\n",
    "### Input Validation\n",
    "1. Ensure that $x_1$ and $x_2$ have the same dimensions.\n",
    "2. Check that $\\sigma > 0$ to avoid division by zero.\n",
    "\n",
    "### Test Cases\n",
    "1. **Identical Inputs**:\n",
    "   - For $x_1 = x_2$, the output should be $1$, as $\\|x_1 - x_2\\| = 0$.\n",
    "2. **Distant Points**:\n",
    "   - For large distances between $x_1$ and $x_2$, the output should approach $0$.\n",
    "3. **Effect of $\\sigma$**:\n",
    "   - For small $\\sigma$, non-identical points should yield values close to $0$.\n",
    "   - For large $\\sigma$, the kernel should produce values close to $1$ for most points.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7be180-28d6-492d-96e0-99bbcad74d8d",
   "metadata": {},
   "source": [
    "# Understanding the Initialization of the Kernel SVM\n",
    "\n",
    "## Purpose of `__init__`\n",
    "The `__init__` method initializes the key parameters and attributes of the Kernel SVM model. These parameters define the behavior of the model during training and prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Parameters and Their Roles\n",
    "\n",
    "### 1. `kernel`\n",
    "- **Description**: The kernel function (e.g., RBF kernel) computes the similarity between data points.\n",
    "- **Mathematical Role**: Defines the transformation to a higher-dimensional space:\n",
    "  $K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$\n",
    "\n",
    "### 2. `C` (Regularization Parameter)\n",
    "- **Description**: Balances the trade-off between margin size and misclassification.\n",
    "- **Effect**:\n",
    "  - Large $C$: Penalizes misclassifications heavily, resulting in a smaller margin.\n",
    "  - Small $C$: Allows more misclassifications but increases the margin size, improving generalization.\n",
    "- **Mathematical Constraints**:\n",
    "  $0 \\leq \\alpha_i \\leq C$\n",
    "\n",
    "### 3. `lr` (Learning Rate)\n",
    "- **Description**: Determines the step size during gradient descent updates.\n",
    "- **Mathematical Role**:\n",
    "  Gradient descent updates the model's parameters iteratively:\n",
    "  $\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\cdot \\nabla J(\\theta^{(t)})$\n",
    "  Where:\n",
    "  - $\\eta$ is the learning rate (`lr`).\n",
    "  - $J(\\theta)$ is the objective function (e.g., hinge loss).\n",
    "\n",
    "### 4. `num_iters` (Number of Iterations)\n",
    "- **Description**: The number of steps in gradient descent.\n",
    "- **Effect**:\n",
    "  - More iterations refine the parameters but increase computational cost.\n",
    "  - Too few iterations may result in suboptimal training.\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks for Implementation\n",
    "\n",
    "### 1. Store the Parameters\n",
    "- Save `kernel`, `C`, `lr`, and `num_iters` as attributes of the class for later use.\n",
    "\n",
    "### 2. Validate Inputs\n",
    "- Ensure $C > 0$, $lr > 0$, and `num_iters` is a positive integer.\n",
    "- Check that `kernel` is a callable function (e.g., RBF kernel).\n",
    "\n",
    "### 3. Extendability\n",
    "- Prepare for additional attributes, such as:\n",
    "  - Support vectors\n",
    "  - Lagrange multipliers ($\\alpha$)\n",
    "  - Bias term ($b$)\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Context: Regularization and Gradient Descent\n",
    "\n",
    "### Regularization with $C$\n",
    "The optimization problem in the dual form of SVM:\n",
    "$\\min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) - \\sum_{i=1}^n \\alpha_i$\n",
    "Subject to:\n",
    "$0 \\leq \\alpha_i \\leq C, \\quad \\sum_{i=1}^n \\alpha_i y_i = 0$\n",
    "- $C$ controls the penalty for misclassifications, limiting the size of $\\alpha_i$.\n",
    "\n",
    "### Gradient Descent\n",
    "The iterative update for gradient descent:\n",
    "$\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\cdot \\nabla J(\\theta^{(t)})$\n",
    "Where:\n",
    "- $\\eta$ is the learning rate.\n",
    "- $\\nabla J(\\theta)$ is the gradient of the loss function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab349f-9d0f-4573-9c73-1471fd3c31f4",
   "metadata": {},
   "source": [
    "# Implementing the `fit` Method for Kernel SVM\n",
    "\n",
    "## **Mathematical Foundations**\n",
    "\n",
    "### 1. **Dual Form of SVM**\n",
    "The optimization problem for the dual form of SVM is:\n",
    "$\n",
    "\\min_{\\alpha} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j) - \\sum_{i=1}^n \\alpha_i\n",
    "$\n",
    "Subject to:\n",
    "$\n",
    "0 \\leq \\alpha_i \\leq C \\quad \\text{and} \\quad \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "$\n",
    "- $\\alpha_i$: Lagrange multipliers associated with the constraints.\n",
    "- $K(x_i, x_j)$: Kernel function that computes the similarity between points.\n",
    "- $C$: Regularization parameter that controls the margin.\n",
    "\n",
    "The dual formulation uses the kernel trick, which allows computations to occur in the transformed feature space implicitly.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Gradient Descent**\n",
    "To solve the dual problem, we iteratively update the Lagrange multipliers $\\alpha$ and the bias term $b$ using gradient descent.\n",
    "\n",
    "#### Decision Function:\n",
    "The decision function for SVM is:\n",
    "$\n",
    "f(x) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x) + b\n",
    "$\n",
    "- **Condition**: The update for $\\alpha_i$ depends on whether the sample satisfies:\n",
    "  $\n",
    "  y_i f(x_i) \\geq 1\n",
    "  $\n",
    "  - If $y_i f(x_i) < 1$: The sample is misclassified or within the margin, so it contributes to the loss.\n",
    "  - If $y_i f(x_i) \\geq 1$: The sample satisfies the margin, so $\\alpha_i$ only minimizes regularization.\n",
    "\n",
    "#### Gradient Updates:\n",
    "- If $y_i f(x_i) < 1$, update $\\alpha_i$ to minimize the hinge loss and enforce the margin:\n",
    "  $\n",
    "  \\alpha_i \\gets \\alpha_i + \\eta \\left(C (1 - y_i f(x_i)) - \\alpha_i \\right)\n",
    "  $\n",
    "- If $y_i f(x_i) \\geq 1$, update $\\alpha_i$ to minimize regularization:\n",
    "  $\n",
    "  \\alpha_i \\gets \\alpha_i + \\eta (-\\alpha_i)\n",
    "  $\n",
    "\n",
    "#### Bias Term Update:\n",
    "The bias term $b$ ensures the decision boundary remains optimal:\n",
    "$\n",
    "b \\gets b + \\eta \\sum_{i=1}^n (y_i - \\sum_{j=1}^n \\alpha_j y_j K(x_j, x_i))\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## **Steps for Implementation**\n",
    "\n",
    "### 1. **Initialize Parameters**:\n",
    "- Set $\\alpha$ (Lagrange multipliers) to zero for all samples.\n",
    "- Initialize $b$ (bias) to zero.\n",
    "\n",
    "### 2. **Compute the Kernel Matrix**:\n",
    "- Construct the kernel matrix $K$, where $K[i, j] = K(x_i, x_j)$.\n",
    "- This precomputes pairwise similarities for efficiency.\n",
    "\n",
    "### 3. **Perform Gradient Descent**:\n",
    "- For each iteration and each sample:\n",
    "  - Compute the decision function:\n",
    "    $\n",
    "    f(x_i) = \\sum_{j=1}^n \\alpha_j y_j K(x_j, x_i) + b\n",
    "    $\n",
    "  - Check the margin condition $y_i f(x_i) < 1$ and update $\\alpha_i$ accordingly.\n",
    "- After updating all $\\alpha_i$, adjust the bias term $b$.\n",
    "\n",
    "### 4. **Stop Condition**:\n",
    "- Iterate for a fixed number of steps (`num_iters`), or until convergence.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159aa96-1c57-4f4d-8731-06552703d66d",
   "metadata": {},
   "source": [
    "# Computing the Kernel Matrix in Kernel SVM\n",
    "\n",
    "## **What is the Kernel Matrix?**\n",
    "The kernel matrix $K$ is an $n \\times n$ matrix where each entry $K[i, j]$ represents the similarity between sample $i$ and sample $j$ in the input space. For a kernel function $K(x_i, x_j)$ (e.g., RBF kernel):\n",
    "$$\n",
    "K[i, j] = K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "### **Why Do We Need the Kernel Matrix?**\n",
    "1. The kernel matrix allows us to compute similarities between all training samples in one step.\n",
    "2. It encapsulates the \"geometry\" of the data in the transformed (implicit) feature space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematics Behind the Kernel Matrix**\n",
    "\n",
    "### Entry-Wise Calculation\n",
    "For every pair of samples $x_i$ and $x_j$:\n",
    "1. **Kernel Function**:\n",
    "   - Compute $K(x_i, x_j)$ using the provided kernel function (e.g., RBF kernel):\n",
    "     $$\n",
    "     K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\n",
    "     $$\n",
    "   - This measures the similarity between $x_i$ and $x_j$.\n",
    "\n",
    "2. **Matrix Construction**:\n",
    "   - Build a symmetric matrix $K$ where:\n",
    "     - $K[i, j] = K(x_i, x_j)$\n",
    "     - $K[j, i] = K(x_j, x_i)$ (symmetry holds for kernels like RBF).\n",
    "\n",
    "### Computational Complexity\n",
    "- Computing $K$ involves $n^2$ evaluations of the kernel function, where $n$ is the number of samples.\n",
    "\n",
    "---\n",
    "\n",
    "## **Steps for Implementation**\n",
    "\n",
    "1. **Initialize the Kernel Matrix**:\n",
    "   - Create an $n \\times n$ matrix initialized to zeros.\n",
    "\n",
    "2. **Compute Pairwise Kernel Values**:\n",
    "   - Iterate over all pairs of samples $i$ and $j$.\n",
    "   - Compute the kernel function $K(x_i, x_j)$ and store the result in $K[i, j]$.\n",
    "\n",
    "3. **Symmetry Optimization** (Optional):\n",
    "   - Since $K[i, j] = K[j, i]$, you can reduce computation by calculating only the upper triangle of the matrix and mirroring it.\n",
    "\n",
    "---\n",
    "\n",
    "## **What to Do in Code**\n",
    "\n",
    "1. Create an empty matrix `K` of shape `(n_samples, n_samples)` initialized to zeros.\n",
    "2. Loop over each pair of samples `i` and `j`:\n",
    "   - Compute the kernel function for `X[i]` and `X[j]`.\n",
    "   - Store the result in `K[i, j]`.\n",
    "3. Optionally optimize using the symmetry of the matrix.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b06435-58ed-4f26-a551-41bd704de04d",
   "metadata": {},
   "source": [
    "# Gradient Descent Updates in Kernel SVM\n",
    "\n",
    "## **Key Steps**\n",
    "\n",
    "### 1. Compute the Decision Function\n",
    "The decision function determines how well the current model classifies a sample:\n",
    "$$\n",
    "f(x_i) = \\sum_{j=1}^n \\alpha_j y_j K(x_j, x_i) + b\n",
    "$$\n",
    "\n",
    "### 2. Check the Margin Condition\n",
    "For each sample, check:\n",
    "$$\n",
    "y_i f(x_i) < 1\n",
    "$$\n",
    "- If `True`: The sample contributes to the hinge loss, and we update $\\alpha[i]$.\n",
    "- If `False`: The sample satisfies the margin, and we only adjust $\\alpha[i]$ for regularization.\n",
    "\n",
    "### 3. Update Rules\n",
    "- **For $\\alpha[i]$**:\n",
    "  - If $y_i f(x_i) < 1$:\n",
    "    $$\n",
    "    \\alpha_i \\gets \\alpha_i + \\eta \\left(C \\cdot (1 - y_i f(x_i)) - \\alpha_i\\right)\n",
    "    $$\n",
    "  - Otherwise:\n",
    "    $$\n",
    "    \\alpha_i \\gets \\alpha_i + \\eta \\cdot (-\\alpha_i)\n",
    "    $$\n",
    "\n",
    "- **For Bias Term $b$**:\n",
    "  Update the bias to minimize the overall loss:\n",
    "  $$\n",
    "  b \\gets b + \\eta \\sum_{i=1}^n \\left(y_i - \\sum_{j=1}^n \\alpha_j y_j K(x_j, x_i)\\right)\n",
    "  $$\n",
    "\n",
    "### 4. Repeat for Multiple Iterations\n",
    "Perform the updates over `num_iters` iterations to refine the parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## **Steps for Implementation**\n",
    "\n",
    "1. **Outer Loop (Iterations)**:\n",
    "   - Iterate `num_iters` times to refine the model.\n",
    "\n",
    "2. **Inner Loop (Samples)**:\n",
    "   - For each sample, compute the decision function $f(x_i)$.\n",
    "   - Check the margin condition and update $\\alpha[i]$.\n",
    "\n",
    "3. **Bias Update**:\n",
    "   - After updating all $\\alpha$, adjust the bias term $b$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83afa87e-08b3-4c06-8e8c-325601a0e57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the Gaussian RBF Kernel\n",
    "def rbf_kernel(x1, x2, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian RBF Kernel between two vectors.\n",
    "    K(x1, x2) = exp(-||x1 - x2||^2 / (2 * sigma^2))\n",
    "    \"\"\"\n",
    "    return np.exp(-(np.linalg.norm(x1-x2))**2/((2*sigma**2))) # Calculating the Gaussian RBF Kernel\n",
    "\n",
    "# SVM with Kernel Trick\n",
    "class KernelSVM:\n",
    "    def __init__(self, kernel, C=1.0, lr=0.001, num_iters=1000):\n",
    "        self.kernel = kernel # saving the kernel\n",
    "        self.C = C #Saving the regularization parameter\n",
    "        self.lr=lr #Saving the learning rate\n",
    "        self.num_iters=num_iters #Saving the maximum number of iterations\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the SVM model using gradient descent.\n",
    "        X: Input features (n_samples, n_features)\n",
    "        y: Labels (-1 or 1)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0] #Number of samples is the rows of X\n",
    "        self.alpha = np.zeros(n_samples) # Array of zeroes for Lagrange Multipliers for all Samples\n",
    "        self.bias= 0 #bias term set to 0\n",
    "        \n",
    "        assert len(X.shape) == 2, \"X must be a 2D array\"\n",
    "        assert X.shape[0] == len(y), \"Number of samples in X and y must match\"\n",
    "\n",
    "        matrix = np.zeros((n_samples,n_samples)) # The matrix of zeroes\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            for j in range(i, n_samples):  # Start at `i` to compute only the upper triangle\n",
    "                matrix[i, j] = self.kernel(X[i], X[j]) # Calculating the kernel for half the matrix\n",
    "                matrix[j, i] = matrix[i, j]  # Exploit symmetry: The other half the matrix is the same as the upper half means n*(n+1)/2 instead of n^2 calculations\n",
    "\n",
    "        # Gradient Descent\n",
    "        for _ in range(self.num_iters):\n",
    "            for i in range(n_samples):\n",
    "                decision_value = np.sum(self.alpha*y*matrix[:,i])+self.bias \n",
    "                final = y[i]*decision_value\n",
    "\n",
    "                if final<1:\n",
    "                    self.alpha[i]+=self.lr*(self.C*(1-final))-self.alpha[i] #Update of the alpha term\n",
    "                else:\n",
    "                    self.alpha[i]+=self.lr*(-self.alpha[i]) # Update of the alpha term\n",
    "\n",
    "            self.bias += self.lr * np.sum(y - np.sum(self.alpha * y[:, None] * matrix, axis=0))# Update of the bias term\n",
    "                \n",
    "        \n",
    "\n",
    "    def predict(self, X_train, X_test):\n",
    "        \"\"\"\n",
    "        Predict the class of each test sample.\n",
    "        X_train: Training data (for computing kernel with test points)\n",
    "        X_test: Test data\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            prediction = 0\n",
    "            for i in range(len(X_train)):\n",
    "               prediction += self.alpha[i] * y_train[i] * self.kernel(X_train[i], x)\n",
    "            y_pred.append(np.sign(prediction + self.bias))\n",
    "        return np.array(y_pred)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    X_train = np.array([[2, 3], [1, 1], [2, 2], [4, 5], [5, 6], [1, 0]])\n",
    "    y_train = np.array([1, -1, -1, 1, 1, -1])\n",
    "\n",
    "    # Define the kernel SVM with RBF kernel\n",
    "    svm = KernelSVM(kernel=rbf_kernel, C=1.0, lr=0.001, num_iters=1000)\n",
    "\n",
    "    # Train the model\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model on new data\n",
    "    X_test = np.array([[2, 2], [3, 3], [5, 5], [0, 0]])\n",
    "    y_pred = svm.predict(X_train, X_test)\n",
    "\n",
    "    # Output the predictions\n",
    "    print(\"Predictions:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f196ed-2a4f-4616-bedb-c682d4c95948",
   "metadata": {},
   "source": [
    "# Changes SVM for Proper Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "058e3d32-82c4-46e2-abd9-2179383f3440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [-1.  1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the Gaussian RBF Kernel\n",
    "def rbf_kernel(x1, x2, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian RBF Kernel between two vectors.\n",
    "    K(x1, x2) = exp(-||x1 - x2||^2 / (2 * sigma^2))\n",
    "    \"\"\"\n",
    "    return np.exp(-np.linalg.norm(x1 - x2)**2 / (2 * sigma**2))\n",
    "\n",
    "# SVM with Kernel Trick\n",
    "class KernelSVM:\n",
    "    def __init__(self, kernel, C=1.0, lr=0.001, num_iters=1000):\n",
    "        self.kernel = kernel  # Kernel function\n",
    "        self.C = C  # Regularization parameter\n",
    "        self.lr = lr  # Learning rate\n",
    "        self.num_iters = num_iters  # Number of iterations\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the SVM model using gradient descent.\n",
    "        X: Input features (n_samples, n_features)\n",
    "        y: Labels (-1 or 1)\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        self.alpha = np.zeros(n_samples)  # Lagrange multipliers\n",
    "        self.bias = 0  # Bias term\n",
    "\n",
    "        # Compute the kernel matrix\n",
    "        matrix = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(i, n_samples):\n",
    "                matrix[i, j] = self.kernel(X[i], X[j])\n",
    "                matrix[j, i] = matrix[i, j]  # Symmetry of the kernel\n",
    "\n",
    "        # Gradient descent for alpha and bias\n",
    "        for _ in range(self.num_iters):\n",
    "            for i in range(n_samples):\n",
    "                decision_value = np.sum(self.alpha * y * matrix[:, i]) + self.bias\n",
    "                margin = y[i] * decision_value\n",
    "\n",
    "                # Subgradient update\n",
    "                if margin < 1:  # Misclassified or on the margin\n",
    "                    self.alpha[i] += self.lr * (self.C - margin)\n",
    "                else:  # Correct classification\n",
    "                    self.alpha[i] += self.lr * (-self.alpha[i])\n",
    "\n",
    "                # Enforce box constraints: 0 <= alpha_i <= C\n",
    "                self.alpha[i] = max(0, min(self.alpha[i], self.C))\n",
    "\n",
    "            # Update bias term (simple averaging over support vectors)\n",
    "            support_vector_indices = np.where((self.alpha > 1e-5) & (self.alpha < self.C))[0]\n",
    "            if len(support_vector_indices) > 0:\n",
    "                self.bias = np.mean(\n",
    "                    y[support_vector_indices]\n",
    "                    - np.sum(\n",
    "                        self.alpha * y[:, None] * matrix[:, support_vector_indices],\n",
    "                        axis=0,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def predict(self, X_train, X_test):\n",
    "        \"\"\"\n",
    "        Predict the class of each test sample.\n",
    "        X_train: Training data (for computing kernel with test points)\n",
    "        X_test: Test data\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            prediction = 0\n",
    "            for i in range(len(X_train)):\n",
    "                prediction += self.alpha[i] * y_train[i] * self.kernel(X_train[i], x)\n",
    "            y_pred.append(np.sign(prediction + self.bias))\n",
    "        return np.array(y_pred)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data\n",
    "    X_train = np.array([[2, 3], [1, 1], [2, 2], [4, 5], [5, 6], [1, 0]])\n",
    "    y_train = np.array([1, -1, -1, 1, 1, -1])\n",
    "\n",
    "    # Define the kernel SVM with RBF kernel\n",
    "    svm = KernelSVM(kernel=rbf_kernel, C=1.0, lr=0.001, num_iters=1000)\n",
    "\n",
    "    # Train the model\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model on new data\n",
    "    X_test = np.array([[2, 2], [3, 3], [5, 5], [0, 0]])\n",
    "    y_pred = svm.predict(X_train, X_test)\n",
    "\n",
    "    # Output the predictions\n",
    "    print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65c25c-d501-4347-ac28-9925319c331f",
   "metadata": {},
   "source": [
    "# Understanding the Predictions\n",
    "\n",
    "## **Predictions Output**\n",
    "The model output:\n",
    "Predictions: [-1. 1. 1. -1.]\n",
    "\n",
    "\n",
    "This means that for the given test dataset, the model predicts **class 1** for some test points and **class -1** for others. \n",
    "\n",
    "---\n",
    "\n",
    "## **What Does It Mean?**\n",
    "\n",
    "1. **Class Labels**:\n",
    "   - Your model is a binary classifier, trained with labels `-1` and `1`.\n",
    "   - The predictions `[-1. 1. 1. -1.]` indicate that the model predicts **some to class -1 and some to class 1**.\n",
    "\n",
    "2. **Decision Boundary**:\n",
    "   - The SVM decision boundary is designed to separate the two classes based on the training data.\n",
    "   - A prediction of `1` for a test sample means that the sample lies **on or within the region classified as class 1**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3d840-bae5-4701-a26d-c74807ae53bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
