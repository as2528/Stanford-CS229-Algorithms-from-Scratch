{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83b40e9-d862-4ca6-a385-01e3e2b017c7",
   "metadata": {},
   "source": [
    "# Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201ac39-fa11-4a8c-8024-dec422015f0d",
   "metadata": {},
   "source": [
    "## Classes in the Iris Dataset\n",
    "\n",
    "In the Iris dataset, the target variable we are trying to predict is the species of the Iris flower. There are three possible classes:\n",
    "\n",
    "1. **Iris-setosa**: Represented by class label `0`.\n",
    "2. **Iris-versicolor**: Represented by class label `1`.\n",
    "3. **Iris-virginica**: Represented by class label `2`.\n",
    "\n",
    "### Features:\n",
    "Each flower is described by four features:\n",
    "- **Sepal length** (cm)\n",
    "- **Sepal width** (cm)\n",
    "- **Petal length** (cm)\n",
    "- **Petal width** (cm)\n",
    "\n",
    "### Goal:\n",
    "The Naive Bayes classifier will use these four features to predict which of the three classes a given flower belongs to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d34a65-4a96-4b6a-aff8-8decf403b32e",
   "metadata": {},
   "source": [
    "# Naive Bayes: Explanation and Math\n",
    "\n",
    "## 1. Overview of Naive Bayes\n",
    "Naive Bayes is a **probabilistic classifier** based on **Bayes' Theorem**. It is called \"Naive\" because it makes a strong assumption: it assumes that all features are **conditionally independent** given the class label.\n",
    "\n",
    "The goal of Naive Bayes is to classify a new observation $X$ into one of several classes $C_k$ by finding the class that maximizes the **posterior probability** $P(C_k | X)$.\n",
    "\n",
    "According to **Bayes' Theorem**:\n",
    "\n",
    "$$\n",
    "P(C_k | X) = \\frac{P(X | C_k) \\cdot P(C_k)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(C_k | X)$ is the **posterior probability**, i.e., the probability of class $C_k$ given the feature vector $X$.\n",
    "- $P(X | C_k)$ is the **likelihood**, i.e., the probability of the feature vector $X$ given the class $C_k$.\n",
    "- $P(C_k)$ is the **prior probability** of class $C_k$.\n",
    "- $P(X)$ is the **evidence**, which is the total probability of the features $X$.\n",
    "\n",
    "Since $P(X)$ is the same for all classes, we don’t need to compute it to make predictions. Instead, we only need to maximize the numerator, $P(X | C_k) \\cdot P(C_k)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gaussian Naive Bayes\n",
    "\n",
    "In the case of **Gaussian Naive Bayes**, we assume that the features follow a **Gaussian (Normal) distribution** for each class. For a given feature $X_i$ and class $C_k$, the likelihood $P(X_i | C_k)$ is modeled by the **Gaussian probability density function (PDF)**:\n",
    "\n",
    "$$\n",
    "P(X_i | C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}} \\exp\\left( -\\frac{(X_i - \\mu_k)^2}{2\\sigma_k^2} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mu_k$ is the **mean** of feature $X_i$ for class $C_k$.\n",
    "- $\\sigma_k^2$ is the **variance** of feature $X_i$ for class $C_k$.\n",
    "\n",
    "For each class $C_k$, the algorithm computes:\n",
    "- The **prior probability** $P(C_k)$, which is the fraction of samples belonging to class $C_k$.\n",
    "- The **mean** $\\mu_k$ and **variance** $\\sigma_k^2$ of each feature for class $C_k$ based on the training data.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Bayes' Theorem in Action\n",
    "\n",
    "To make a prediction, the Naive Bayes algorithm computes the **posterior probability** for each class $C_k$. This is done by combining the prior probability $P(C_k)$ and the likelihoods $P(X_i | C_k)$ for each feature $X_i$.\n",
    "\n",
    "### Posterior Probability:\n",
    "\n",
    "$$\n",
    "P(C_k | X) \\propto P(C_k) \\cdot \\prod_{i=1}^{d} P(X_i | C_k)\n",
    "$$\n",
    "\n",
    "Where $d$ is the number of features in the feature vector $X$.\n",
    "\n",
    "However, to avoid **numerical underflow** when multiplying many small probabilities, we usually take the **logarithm** of the probabilities:\n",
    "\n",
    "$$\n",
    "\\log(P(C_k | X)) = \\log(P(C_k)) + \\sum_{i=1}^{d} \\log(P(X_i | C_k))\n",
    "$$\n",
    "\n",
    "This turns the product into a sum, which is easier to compute and avoids underflow issues.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary of the Algorithm:\n",
    "\n",
    "### Training (fitting the model):\n",
    "1. **Compute class priors**: For each class $C_k$, compute $P(C_k)$, the proportion of training examples in class $C_k$.\n",
    "2. **Compute the mean and variance** for each feature in each class. For each class $C_k$, compute $\\mu_k$ and $\\sigma_k^2$ for each feature.\n",
    "\n",
    "### Prediction:\n",
    "1. For each new observation (test sample):\n",
    "   - Compute the **log posterior probability** for each class:\n",
    "     $$\n",
    "     \\log(P(C_k | X)) = \\log(P(C_k)) + \\sum_{i=1}^{d} \\log(P(X_i | C_k))\n",
    "     $$\n",
    "   - Choose the class $C_k$ that maximizes the posterior probability.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Example Walkthrough:\n",
    "\n",
    "Let’s walk through a simple example to illustrate how the Naive Bayes classifier works:\n",
    "\n",
    "### Training Data:\n",
    "We have 4 samples with 2 features, and two possible classes $C_1$ and $C_2$:\n",
    "\n",
    "| Feature 1 | Feature 2 | Class |\n",
    "|-----------|-----------|-------|\n",
    "| 5.0       | 3.5       | 0     |\n",
    "| 6.0       | 3.0       | 0     |\n",
    "| 5.5       | 2.5       | 1     |\n",
    "| 6.5       | 3.5       | 1     |\n",
    "\n",
    "- Compute the **mean** and **variance** for each feature in each class.\n",
    "- Compute the **prior** for each class.\n",
    "\n",
    "### Prediction:\n",
    "Given a new sample with feature values $X_1 = 5.5$ and $X_2 = 3.0$, we compute the **posterior probability** for each class:\n",
    "\n",
    "- Compute the **log prior** for each class.\n",
    "- Compute the **Gaussian likelihood** for each feature given each class.\n",
    "- Add the log prior and the sum of the log likelihoods to compute the log posterior for each class.\n",
    "- **Classify** the new sample based on the class with the highest log posterior probability.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Why is it \"Naive\"?\n",
    "The key assumption in Naive Bayes is that all features are **conditionally independent** given the class. This means that the value of one feature does not influence the value of another feature within the same class. This assumption is rarely true in real-world data, but Naive Bayes still performs surprisingly well, especially for **text classification** and **spam filtering**.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Advantages and Limitations:\n",
    "- **Advantages**:\n",
    "  - Simple and easy to implement.\n",
    "  - Works well with small datasets.\n",
    "  - Particularly effective for **text classification**.\n",
    "  - Computationally efficient (requires just one pass over the training data).\n",
    "\n",
    "- **Limitations**:\n",
    "  - Assumes **conditional independence** of features, which may not hold in real data.\n",
    "  - Can struggle with datasets where features are highly correlated.\n",
    "  - For continuous features, Gaussian Naive Bayes assumes a **normal distribution**, which may not be accurate in some cases.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "Naive Bayes is a simple yet powerful classifier that makes predictions based on the application of Bayes' Theorem, using strong (naive) independence assumptions. Despite these assumptions, Naive Bayes often works well in practice and is particularly useful for high-dimensional problems like text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f99279-c21f-4c42-8975-cc3485356a17",
   "metadata": {},
   "source": [
    "# Naive Bayes - Step-by-step Implementation\n",
    "\n",
    "## 1. `fit` Method:\n",
    "The `fit` method computes the **mean**, **variance**, and **prior** for each class.\n",
    "\n",
    "- **Steps:**\n",
    "  - Identify unique classes in `y` (target labels).\n",
    "  - For each class:\n",
    "    - Extract the subset of the training data corresponding to that class.\n",
    "    - Compute the mean and variance of each feature within that subset.\n",
    "    - Compute the prior for that class (i.e., the proportion of data points in that class).\n",
    "\n",
    "- **Math:**\n",
    "  - Mean for class $C_k$:\n",
    "    $$\n",
    "    \\mu_k = \\frac{1}{n_k} \\sum_{i \\in C_k} X_i\n",
    "    $$\n",
    "  - Variance for class $C_k$:\n",
    "    $$\n",
    "    \\sigma_k^2 = \\frac{1}{n_k} \\sum_{i \\in C_k} (X_i - \\mu_k)^2\n",
    "    $$\n",
    "  - Prior for class $C_k$:\n",
    "    $$\n",
    "    P(C_k) = \\frac{n_k}{n}\n",
    "    $$\n",
    "  where $n_k$ is the number of training samples for class $C_k$, and $n$ is the total number of samples.\n",
    "\n",
    "**What to do:**\n",
    "- Store the computed means, variances, and priors for each class in class attributes, such as `self.mean`, `self.var`, and `self.priors`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `predict` Method:\n",
    "The `predict` method uses the trained model to predict the class labels for a set of test samples.\n",
    "\n",
    "- **Steps:**\n",
    "  - For each sample in the test set, compute the posterior probability for each class using the `_predict` method.\n",
    "  - Select the class with the highest posterior probability.\n",
    "\n",
    "- **Math:**\n",
    "  For each class $C_k$, calculate:\n",
    "  $$\n",
    "  P(C_k | X) \\propto P(C_k) \\prod_{i=1}^{d} P(X_i | C_k)\n",
    "  $$\n",
    "  You’ll sum the log of these probabilities to avoid underflow.\n",
    "\n",
    "**What to do:**\n",
    "- Iterate over each test sample, call `_predict` for each, and return the predicted class for each sample.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. `_predict` Method:\n",
    "This method computes the **posterior probability** for each class given a test sample, and then selects the class with the highest probability.\n",
    "\n",
    "- **Steps:**\n",
    "  - For each class, compute the logarithm of the prior probability $P(C_k)$.\n",
    "  - Add the sum of the logarithms of the Gaussian probability densities for each feature using the `_pdf` method.\n",
    "  - Return the class with the highest posterior probability.\n",
    "\n",
    "- **Math:**\n",
    "  Posterior probability for class $C_k$:\n",
    "  $$\n",
    "  \\log(P(C_k | X)) = \\log(P(C_k)) + \\sum_{i=1}^{d} \\log(P(X_i | C_k))\n",
    "  $$\n",
    "\n",
    "**What to do:**\n",
    "- Store these posterior probabilities in a list, and use `np.argmax` to return the class with the highest value.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. `_pdf` Method:\n",
    "The `_pdf` method computes the **Gaussian probability density function** for a given feature value, given a class.\n",
    "\n",
    "- **Steps:**\n",
    "  - For each feature in the sample, compute the Gaussian probability density using the class-specific mean and variance.\n",
    "  - Return the probability for that feature.\n",
    "\n",
    "- **Math:**\n",
    "  The Gaussian probability density function is:\n",
    "  $$\n",
    "  P(X_i | C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}} \\exp\\left( -\\frac{(X_i - \\mu_k)^2}{2\\sigma_k^2} \\right)\n",
    "  $$\n",
    "\n",
    "**What to do:**\n",
    "- Use the mean and variance for the class (calculated in the `fit` method) to compute the likelihood of the feature value using the above formula.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c798fe8-d859-49dc-82ec-d8a7d1e0dcee",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be1d554b-bf29-464e-8b6b-a0391305ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb117c6-8602-4d13-9b09-20b51a6b99de",
   "metadata": {},
   "source": [
    "Step 2: Load the Iris Dataset\n",
    "\n",
    "We'll load the Iris dataset into a pandas DataFrame. Since you mentioned the dataset is from UCI, you can directly read it from the file if you have it locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "575c0daf-e034-4655-b24e-9465e147ca0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from the correct file path with the extension\n",
    "df = pd.read_csv(r'C:\\Users\\Machine-Learning\\Downloads\\iris\\iris.data', header=None)\n",
    "\n",
    "# Assign column names to the dataset\n",
    "df.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac97917-881d-4718-b102-0f22c91852e7",
   "metadata": {},
   "source": [
    "Step 3: Prepare the Data\n",
    "\n",
    "Convert the class labels into numerical values and split the data into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6aa488-67b0-4902-bae8-925cbb5bf6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class labels to numerical values\n",
    "df['class'] = df['class'].map({'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4516c80-26d4-4eea-843d-cdc22d469840",
   "metadata": {},
   "source": [
    "Step 4: Implement the Gaussian Naive Bayes Classifier\n",
    "\n",
    "Now, let's implement the Naive Bayes classifier. We'll create a class that fits the model to the data and makes predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60251d7c-7688-4862-8fe6-350e7c41c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Naive Bayes model according to the training data.\n",
    "        \"\"\"\n",
    "        \n",
    "        #Picking the indexes of each class from the y array\n",
    "        zero_idx=np.where(y==0)\n",
    "        ones_idx=np.where(y==1)\n",
    "        two_idx=np.where(y==2)\n",
    "\n",
    "        #Picking the rows corresponding to each class from the X Matrix\n",
    "        X_zeroes = X[zero_idx[0],:]\n",
    "        X_ones = X[ones_idx[0],:]\n",
    "        X_twoes = X[two_idx[0],:]\n",
    "\n",
    "        #Finding the average of the features\n",
    "\n",
    "        #This code uses the np library to calculate the averages. This will work, but I wanted to hand code it. \n",
    "  #      X0_avg = np.average(X_zeroes,axis=0) #if you want the average for each feature (i.e., column) across all samples for a particular class, you should specify axis=0 in np.average, which averages each feature independently.\n",
    "   #     X1_avg = np.average(X_ones,axis=0)#if you want the average for each feature (i.e., column) across all samples for a particular class, you should specify axis=0 in np.average, which averages each feature independently.\n",
    "    #    X2_avg = np.average(X_twoes,axis=0)#if you want the average for each feature (i.e., column) across all samples for a particular class, you should specify axis=0 in np.average, which averages each feature independently.\n",
    "\n",
    "        # The average of class 0\n",
    "        feature_0=0\n",
    "        feature_1=0\n",
    "        feature_2=0\n",
    "        feature_3=0\n",
    "        \n",
    "        for i in range(0, X_zeroes.shape[0]): #Over the rows loop, i= index of sample\n",
    "            for j in range(0, X_zeroes.shape[1]): #Over the columns loop, j=index of feature\n",
    "\n",
    "                if j==0:\n",
    "                    feature_0+=X_zeroes[i][j] #Each feature 0 is added to the variable for each sample\n",
    "                elif j==1:\n",
    "                    feature_1 += X_zeroes[i][j] #Each feature 0 is added to the variable for each sample\n",
    "                elif j==2:\n",
    "                    feature_2 += X_zeroes[i][j] #Each feature 0 is added to the variable for each sample\n",
    "                elif j==3:\n",
    "                    feature_3 += X_zeroes[i][j] #Each feature 0 is added to the variable for each sample\n",
    "\n",
    "        X0_avg = [feature_0/X_zeroes.shape[0], feature_1/X_zeroes.shape[0], feature_2/X_zeroes.shape[0], feature_3/X_zeroes.shape[0]]\n",
    "\n",
    "        # The average of class 1\n",
    "        feature_0=0\n",
    "        feature_1=0\n",
    "        feature_2=0\n",
    "        feature_3=0\n",
    "        \n",
    "        for i in range(0, X_ones.shape[0]):\n",
    "            for j in range(0, X_ones.shape[1]):\n",
    "\n",
    "                if j==0:\n",
    "                    feature_0+=X_ones[i][j]\n",
    "                elif j==1:\n",
    "                    feature_1 += X_ones[i][j]\n",
    "                elif j==2:\n",
    "                    feature_2 += X_ones[i][j]\n",
    "                elif j==3:\n",
    "                    feature_3 += X_ones[i][j]\n",
    "                    \n",
    "        X1_avg = [feature_0/X_ones.shape[0], feature_1/X_ones.shape[0], feature_2/X_ones.shape[0], feature_3/X_ones.shape[0]]\n",
    "\n",
    "        # The average of class 2\n",
    "        feature_0=0\n",
    "        feature_1=0\n",
    "        feature_2=0\n",
    "        feature_3=0\n",
    "        \n",
    "        for i in range(0, X_twoes.shape[0]):\n",
    "            for j in range(0, X_twoes.shape[1]):\n",
    "\n",
    "                if j==0:\n",
    "                    feature_0+=X_twoes[i][j]\n",
    "                elif j==1:\n",
    "                    feature_1 += X_twoes[i][j]\n",
    "                elif j==2:\n",
    "                    feature_2 += X_twoes[i][j]\n",
    "                elif j==3:\n",
    "                    feature_3 += X_twoes[i][j]\n",
    "                    \n",
    "        X2_avg = [feature_0/X_twoes.shape[0], feature_1/X_twoes.shape[0], feature_2/X_twoes.shape[0], feature_3/X_twoes.shape[0]]\n",
    "                \n",
    "        \n",
    "        #Finding the variance of the features\n",
    "\n",
    "        # The variance of class 0\n",
    "        feature_0=0\n",
    "        feature_1=0\n",
    "        feature_2=0\n",
    "        feature_3=0\n",
    "        \n",
    "        for i in range(0, X_zeroes.shape[0]):\n",
    "            for j in range(0, X_zeroes.shape[1]):\n",
    "\n",
    "                if j==0:\n",
    "                    feature_0+=(X_zeroes[i][j]-X0_avg[0])**2\n",
    "                elif j==1:\n",
    "                    feature_1 += (X_zeroes[i][j]-X0_avg[1])**2\n",
    "                elif j==2:\n",
    "                    feature_2 += (X_zeroes[i][j]-X0_avg[2])**2\n",
    "                elif j==3:\n",
    "                    feature_3 += (X_zeroes[i][j]-X0_avg[3])**2\n",
    "\n",
    "        X0_var = [feature_0/X_zeroes.shape[0], feature_1/X_zeroes.shape[0], feature_2/X_zeroes.shape[0], feature_3/X_zeroes.shape[0]]\n",
    "        \n",
    "        # The variance of class 1\n",
    "        feature_0=0\n",
    "        feature_1=0\n",
    "        feature_2=0\n",
    "        feature_3=0\n",
    "        \n",
    "        for i in range(0, X_ones.shape[0]):\n",
    "            for j in range(0, X_ones.shape[1]):\n",
    "\n",
    "                if j==0:\n",
    "                    feature_0+=(X_ones[i][j]-X1_avg[0])**2\n",
    "                elif j==1:\n",
    "                    feature_1 += (X_ones[i][j]-X1_avg[1])**2\n",
    "                elif j==2:\n",
    "                    feature_2 += (X_ones[i][j]-X1_avg[2])**2\n",
    "                elif j==3:\n",
    "                    feature_3 += (X_ones[i][j]-X1_avg[3])**2\n",
    "\n",
    "        X1_var = [feature_0/X_ones.shape[0], feature_1/X_ones.shape[0], feature_2/X_ones.shape[0], feature_3/X_ones.shape[0]]\n",
    "        \n",
    "        # The variance of class 2\n",
    "        feature_0=0\n",
    "        feature_1=0\n",
    "        feature_2=0\n",
    "        feature_3=0\n",
    "        \n",
    "        for i in range(0, X_twoes.shape[0]):\n",
    "            for j in range(0, X_twoes.shape[1]):\n",
    "\n",
    "                if j==0:\n",
    "                    feature_0+=(X_twoes[i][j]-X2_avg[0])**2\n",
    "                elif j==1:\n",
    "                    feature_1 += (X_twoes[i][j]-X2_avg[1])**2\n",
    "                elif j==2:\n",
    "                    feature_2 += (X_twoes[i][j]-X2_avg[2])**2\n",
    "                elif j==3:\n",
    "                    feature_3 += (X_twoes[i][j]-X2_avg[3])**2\n",
    "\n",
    "        X2_var = [feature_0/X_twoes.shape[0], feature_1/X_twoes.shape[0], feature_2/X_twoes.shape[0], feature_3/X_twoes.shape[0]]\n",
    "        self.mean = np.array([X0_avg, X1_avg, X2_avg])\n",
    "        self.var = np.array([X0_var, X1_var, X2_var])\n",
    "\n",
    "        #Calculating the prior probabilities for each class\n",
    "        prior0 = X_zeroes.shape[0]/X.shape[0]\n",
    "        prior1 = X_ones.shape[0]/X.shape[0]\n",
    "        prior2 = X_twoes.shape[0]/X.shape[0]\n",
    "    \n",
    "        self.priors=(prior0,prior1,prior2)\n",
    "        self.classes = np.unique(y) # The labels 0,1,2 stored as [0,1,2]\n",
    "\n",
    "    \n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform classification on an array of test vectors X.\n",
    "        \"\"\"\n",
    "\n",
    "        # This will store the predicted class for each sample\n",
    "        predicted_classes = []\n",
    "\n",
    "         # Loop through each test sample 'x' in the input 'X'\n",
    "        for x in X:\n",
    "            # Call self._predict(x) to get the predicted class for the sample 'x'\n",
    "            predicted_classes.append(self._predict(x))# Add the predicted class to the list\n",
    "\n",
    "         # Return the list of all predicted classes\n",
    "        return predicted_classes\n",
    "    \n",
    "    \n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Compute the posterior probability of each class and return the class with the highest probability.\n",
    "        \"\"\"\n",
    "        posterior_probabilities = []  # This will hold the log-posteriors for each class\n",
    "\n",
    "        # Loop over each class\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            log_prior = np.log(self.priors[idx])  # Get log-prior for the class\n",
    "\n",
    "        # Get the log-likelihoods for all features\n",
    "            log_likelihoods = self._pdf(idx, x)  # Returns log-likelihoods\n",
    "\n",
    "        # Sum the log-likelihoods for all features\n",
    "            total_log_likelihood = np.sum(log_likelihoods)\n",
    "\n",
    "        # Calculate the log-posterior for this class\n",
    "            log_posterior = log_prior + total_log_likelihood\n",
    "\n",
    "        # Append the log-posterior for this class to the list\n",
    "            posterior_probabilities.append(log_posterior)\n",
    "\n",
    "    # Find the class with the highest log-posterior\n",
    "        best_class = self.classes[np.argmax(posterior_probabilities)]\n",
    "        return best_class\n",
    "\n",
    "\n",
    "        \n",
    "    def _pdf(self, class_idx, x):\n",
    "        \"\"\"\n",
    "        Compute the log of the probability density function of a Gaussian distribution.\n",
    "        \"\"\"\n",
    "    # Retrieve the mean and variance for the class\n",
    "        mean = self.mean[class_idx]\n",
    "        var = self.var[class_idx] + 1e-9  # Add epsilon to variance to prevent division by zero\n",
    "\n",
    "    # Compute the log of the Gaussian probability density function\n",
    "        numerator = - ((x - mean) ** 2) / (2 * var)\n",
    "        denominator = - 0.5 * np.log(2 * np.pi * var)\n",
    "        return numerator + denominator  # Return the log-likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242b1320-dd0a-4c47-a7b9-9445b06ebb65",
   "metadata": {},
   "source": [
    "Step 5: Train and Evaluate the Model\n",
    "\n",
    "Now, train the Naive Bayes classifier using the training data and evaluate its performance on the test set. Explanation\n",
    "\n",
    "fit method: Computes the mean, variance, and prior probabilities for each class.\n",
    "predict method: Uses the fitted parameters to compute the posterior probability of each class for a given test point and predicts the class with the highest probability.\n",
    "_pdf method: Calculates the probability density function for a Gaussian distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91484e65-365c-4505-8ab6-74c20b767591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train your model\n",
    "nb = GaussianNaiveBayes()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce907d-baec-4a6d-9fe3-48f9f3193d9f",
   "metadata": {},
   "source": [
    "# Clarifying `x` in `_predict`\n",
    "\n",
    "In your `_predict` method, the input `x` represents **a single test sample**.\n",
    "\n",
    "## What is `x`?\n",
    "- The variable `x` contains the **features of one test sample**.\n",
    "- For example, in the Iris dataset, `x` will have 4 feature values: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "For instance, if the sample `x` looks like this:\n",
    "\n",
    "`x = [5.1, 3.5, 1.4, 0.2]`\n",
    "\n",
    "\n",
    "This means:\n",
    "- Sepal length = 5.1\n",
    "- Sepal width = 3.5\n",
    "- Petal length = 1.4\n",
    "- Petal width = 0.2\n",
    "\n",
    "## What do you do with `x` in `_predict`?\n",
    "In `_predict`, you need to loop through **each feature** in `x`. \n",
    "- For each feature, you will calculate the **likelihood** using the Gaussian PDF.\n",
    "- The likelihood will be calculated for **each class** (0, 1, and 2) using the `_pdf` method.\n",
    "\n",
    "### Example of Feature Looping in `_predict`\n",
    "- Loop through the features in `x` (sepal length, sepal width, etc.).\n",
    "- For each feature, you will calculate the **log-likelihood** for the current class.\n",
    "\n",
    "## Where does `X` come into play?\n",
    "- In the `predict` method (not `_predict`), `X` is the **entire test dataset** containing all test samples.\n",
    "- `X` contains many samples, and you will loop through them in `predict`.\n",
    "\n",
    "For example:\n",
    "\n",
    "`X = [ [5.1, 3.5, 1.4, 0.2], # Sample 1 [7.0, 3.2, 4.7, 1.4], # Sample 2 ... ]`\n",
    "\n",
    "## How do `x` and `X` differ?\n",
    "- `X` contains **all test samples**.\n",
    "- `x` is a **single test sample** with its features.\n",
    "\n",
    "### Process:\n",
    "1. In `predict`, you loop through all samples in `X`.\n",
    "2. For each sample `x`, you call `_predict`.\n",
    "3. Inside `_predict`, `x` is just one sample, and you loop through its features to calculate the likelihood.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28c27c-0b89-41b4-80d6-915a488141c6",
   "metadata": {},
   "source": [
    "# Step-by-Step Explanation of `_predict`\n",
    "\n",
    "The goal of `_predict` is to compute the **posterior probability** for each class (0, 1, or 2) and return the class with the **highest probability** for a given test sample `x`.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Compute the Log of the Prior Probability\n",
    "\n",
    "For each class $C_k $, start by calculating the **log of the prior**. The prior represents how common the class is in the training data.\n",
    "\n",
    "```python\n",
    "log_prior = np.log(self.priors[idx])\n",
    "```\n",
    "This gives the initial log-prior probability for the current class.\n",
    "\n",
    "---\n",
    "## Step 2: Calculate the Log-Likelihood for Each Feature\n",
    "\n",
    "For each feature in the test sample $x$, calculate how likely that feature value is for the current class $Ck$​. This is the likelihood, and it’s computed using the Gaussian PDF.\n",
    "\n",
    "To calculate the likelihood, call `_pdf` for each feature:\n",
    "\n",
    "\n",
    "```python\n",
    "likelihood = self._pdf(idx, feature)\n",
    "log_likelihood = np.log(likelihood)\n",
    "\n",
    "```\n",
    "The result is the log-likelihood for that feature.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Sum the Log-Likelihoods\n",
    "\n",
    "You need to sum the log-likelihoods for all the features in the sample. This gives you the total likelihood for that class.\n",
    "\n",
    "Initialize a variable to keep track of the running total for log-likelihoods:\n",
    "\n",
    "```python\n",
    "log_likelihood_sum = 0  # Initialize sum of log-likelihoods\n",
    "```\n",
    "Then, inside the loop for each feature:(replaced by vectorized log likelihoods using vectorized operations\n",
    "\n",
    "```python\n",
    "\n",
    "log_likelihood_sum += log_likelihood  # Add each log-likelihood to the sum\n",
    "```\n",
    "\n",
    "This ensures that you accumulate the log-likelihoods for all the features in the sample.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Calculate the Log-Posterior for Each Class\n",
    "\n",
    "Once you have summed the log-likelihoods for all the features, add this sum to the log-prior to get the log-posterior for the current class.\n",
    "\n",
    "```python\n",
    "log_posterior = log_prior + log_likelihood_sum\n",
    "```\n",
    "\n",
    "The log-posterior gives you a total score for each class, which represents how likely the sample belongs to that class.\n",
    "\n",
    "---\n",
    "\n",
    "Once you have computed the log-posterior for all classes, choose the class with the highest score.\n",
    "\n",
    "To do this, store the log-posteriors for each class in a list called `posterior_probabilities`. Then, use `np.argmax` to find the class with the highest log-posterior:\n",
    "\n",
    "```python\n",
    "posterior_probabilities.append(log_posterior)\n",
    "best_class = self.classes[np.argmax(posterior_probabilities)]\n",
    "\n",
    "```\n",
    "Finally, return the class with the highest log-posterior.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Log of the Prior: Start with the log-prior for each class.\n",
    "- Log-Likelihood: For each feature in `x`, compute the log-likelihood using `_pdf`\n",
    "- Sum the Log-Likelihoods: Accumulate the log-likelihoods for each feature.\n",
    "- Log-Posterior: Add the sum of log-likelihoods to the log-prior.\n",
    "- Return the Best Class: Select the class with the highest log-posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec86ed0-6ab8-4411-b8f1-2fa6bf1f6328",
   "metadata": {},
   "source": [
    "# Summary of `_predict` and `_pdf` Methods\n",
    "\n",
    "In implementing the Gaussian Naive Bayes classifier, two crucial methods are `_predict` and `_pdf`. These methods are responsible for computing the posterior probabilities and the likelihoods, respectively. Below is a comprehensive summary of these methods, including the challenges faced during implementation and how they were resolved.\n",
    "\n",
    "---\n",
    "\n",
    "## `_predict` Method\n",
    "\n",
    "### Purpose\n",
    "\n",
    "The `_predict` method computes the posterior probability for each class and returns the class with the highest probability for a given test sample.\n",
    "\n",
    "### Challenges and Solutions\n",
    "\n",
    "- Issue: Taking the logarithm of log-likelihoods resulted in `nan` values because log-likelihoods are already in log-space.\n",
    "\n",
    "- Solution: Removed the unnecessary `np.log` in the `_predict` method when summing log-likelihoods.\n",
    "\n",
    "- Issue: Incorrect looping over features individually, leading to shape mismatches.\n",
    "\n",
    "- Solution: Passed the entire feature vector `x` to the `_pdf` method and handled all features at once, leveraging NumPy's vectorization.\n",
    "\n",
    "## `_pdf` method\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The `_pdf` method computes the log of the Gaussian probability density function (PDF) for each feature given a class. Computing in log-space enhances numerical stability and prevents underflow issues with very small probability values.\n",
    "\n",
    "## Challenges and Solutions\n",
    "\n",
    "- Issue: Variance values being zero or very small, leading to division by zero or extremely large negative numbers.\n",
    " \n",
    "- Solution: Added a small epsilon `(1e-9)` to the variance to prevent division by zero.\n",
    "\n",
    "- Issue: Computing probabilities in the original scale led to underflow issues with very small numbers.\n",
    "\n",
    "- Solution: Performed computations in log-space by directly computing the log of the Gaussian PDF.\n",
    "\n",
    "- Issue: Shape mismatches when subtracting arrays.\n",
    "\n",
    "- Solution: Ensured that `x`, `mean`, and `var` are NumPy arrays of the same shape (converted lists to arrays when necessary).\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Compute in Log-Space: Performing calculations in log-space improves numerical stability, especially when dealing with very small probabilities.\n",
    "- Avoid Redundant Operations: Do not take the logarithm of values that are already in log-space.\n",
    "- Handle Edge Cases: Add small constants (epsilon) to denominators to avoid division by zero.\n",
    "- Use Vectorization: Leveraging NumPy's vectorized operations eliminates the need for explicit loops over features, reducing the potential for errors and improving performance.\n",
    "- Debugging Shape Mismatches: Always check the shapes of arrays when performing element-wise operations to prevent broadcasting errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c0fd1a-1824-468f-accf-0cab24105d87",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Implementing the `_predict` and `_pdf` methods was a significant learning experience that reinforced the importance of numerical stability and careful handling of mathematical operations in machine learning algorithms. By addressing the challenges faced, such as handling small variances and avoiding invalid logarithmic operations, the Gaussian Naive Bayes classifier was successfully implemented with high accuracy.\n",
    "\n",
    "This detailed explanation should serve as a valuable resource for understanding the inner workings of these methods and as a guide for future projects involving probabilistic models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
